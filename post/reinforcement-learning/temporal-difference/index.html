<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <script type="application/javascript" src='http://localhost:1313/js/theme-mode.js'></script>
    <link rel="stylesheet" href='http://localhost:1313/css/frameworks.min.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/github.min.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/github-style.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/light.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/dark.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/syntax.css' />
    <title>Temporal Difference - My New Hugo Site</title>
    
    <link rel="icon" type="image/x-icon" href='/images/github.png'>
    
    <meta name="theme-color" content="#1e2327">

    <link rel="stylesheet" href="http://localhost:1313/css/custom.css">
    <script type="text/javascript" src="http://localhost:1313/js/custom.js"></script>

    
    <meta name="description"
  content="Temporal Difference 增量式MC 时间差分TD 核心差别： MC是根据[s1→终止状态]完整片段的最终回报更新s1的值函数 TD是根据[s1→s2]这一步片段的即时回报值R和s2的估计值函数更新s1的值函数 与DP的对比 DP是全宽备份 TD是样本备份 TD与MC的对比 TD 算法在知道结果之前学习 TD算法在每一步之后都能在线学习 MC算法必须等待最终回报值得到之后才能学习 TD算法即便没有最终结果也能学习 TD算法能够从不完整序列中学习 MC算法仅仅能够从完整序列中学习 TD算法适用于连续性任务和片段性任务 MC算法仅仅适用于片段性任务 TD算法有多个驱动力 MC算法只有奖励值作为更新的驱动力 TD算法有奖励值和状态转移作为更新的驱动力 MC有高方差,零偏差 收敛性较好 (即使采用函数逼近) 对初始值不太敏感 随着样本数量的增加,方差逐渐减少, 趋近于 0 TD 有低方差,和一些偏差 通常比 MC 效率更高 表格法下TD(0)收敛到(函数逼近时不一定) 对初始值更敏感 随着样本数量的增加,偏差逐渐减少,趋近于 0 样本数量有限时，TD的结果与真实结果的偏差比较稳定。MC可能出现巨大偏差。 TD要求环境符合马尔科夫性，MC不要求 自举和采样 自举: 使用随机变量的估计去更新 MC 没有自举 DP 和 TD 都有自举 采样: 通过样本估计期望 MC 和 TD 采样 DP 不采样 TD的优化方法 整体思路是 策略评价&#43;策略提升 策略评价 在策略评价SARSA 公式 算法流程 1:初始化且 2:repeat(对于每个片段) 3: 初始化状态 4: 根据选择一个在处的动作(使用-贪婪策略) 5: repeat(对于片段中每一步) 6: 执行动作，观测 7: 根据选择一个在处的动作(使用-贪婪策略) 8: 9: 10: until 是终止状态 11:until收敛 收敛性 在满足以下条件时,Sarsa 算法收敛到最优的状态动作值函数 策略序列满足GLIE 步长序列是一个Robbins-Monro序列 GLIE 保证了 充分的探索 策略最终收敛到贪婪的策略 Robbins-Monro保证了 步长足够大,足以克服任意初始值 步长足够小,最终收敛 (常量步长不满足) 期望SARSA 减少了由于的选择带来的方差 在相同更新步数时,期望 Sarsa 比 Sarsa 的通用性更好 可以在在策略和离策略中切换 在策略:TD目标值中的中的策略和采样的策略是同一个策略 离策略:TD目标值中的中的策略和采样的策略是不同的策略 一种特殊情况,TD目标值中的策略选择贪婪策略, 采样的策略选用ε-贪婪策略——Q学习 离策略评价Q学习 公式 算法流程 1:初始化且 2:repeat(对于每个片段) 3: 初始化状态 4: repeat(对于片段中每一步) 5: 根据选择一个在处的动作(使用-贪婪策略) 6: 执行动作，观测 7: 8: 9: until 是终止状态 10:until收敛 策略提升 -贪婪策略提升 " />
<meta name="keywords"
  content='reinforcement-learning,, TD' />
<meta name="robots" content="noodp" />
<link rel="canonical" href="http://localhost:1313/post/reinforcement-learning/temporal-difference/" />


<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Temporal Difference - My New Hugo Site" />
<meta name="twitter:description"
  content="Temporal Difference 增量式MC 时间差分TD 核心差别： MC是根据[s1→终止状态]完整片段的最终回报更新s1的值函数 TD是根据[s1→s2]这一步片段的即时回报值R和s2的估计值函数更新s1的值函数 与DP的对比 DP是全宽备份 TD是样本备份 TD与MC的对比 TD 算法在知道结果之前学习 TD算法在每一步之后都能在线学习 MC算法必须等待最终回报值得到之后才能学习 TD算法即便没有最终结果也能学习 TD算法能够从不完整序列中学习 MC算法仅仅能够从完整序列中学习 TD算法适用于连续性任务和片段性任务 MC算法仅仅适用于片段性任务 TD算法有多个驱动力 MC算法只有奖励值作为更新的驱动力 TD算法有奖励值和状态转移作为更新的驱动力 MC有高方差,零偏差 收敛性较好 (即使采用函数逼近) 对初始值不太敏感 随着样本数量的增加,方差逐渐减少, 趋近于 0 TD 有低方差,和一些偏差 通常比 MC 效率更高 表格法下TD(0)收敛到(函数逼近时不一定) 对初始值更敏感 随着样本数量的增加,偏差逐渐减少,趋近于 0 样本数量有限时，TD的结果与真实结果的偏差比较稳定。MC可能出现巨大偏差。 TD要求环境符合马尔科夫性，MC不要求 自举和采样 自举: 使用随机变量的估计去更新 MC 没有自举 DP 和 TD 都有自举 采样: 通过样本估计期望 MC 和 TD 采样 DP 不采样 TD的优化方法 整体思路是 策略评价&#43;策略提升 策略评价 在策略评价SARSA 公式 算法流程 1:初始化且 2:repeat(对于每个片段) 3: 初始化状态 4: 根据选择一个在处的动作(使用-贪婪策略) 5: repeat(对于片段中每一步) 6: 执行动作，观测 7: 根据选择一个在处的动作(使用-贪婪策略) 8: 9: 10: until 是终止状态 11:until收敛 收敛性 在满足以下条件时,Sarsa 算法收敛到最优的状态动作值函数 策略序列满足GLIE 步长序列是一个Robbins-Monro序列 GLIE 保证了 充分的探索 策略最终收敛到贪婪的策略 Robbins-Monro保证了 步长足够大,足以克服任意初始值 步长足够小,最终收敛 (常量步长不满足) 期望SARSA 减少了由于的选择带来的方差 在相同更新步数时,期望 Sarsa 比 Sarsa 的通用性更好 可以在在策略和离策略中切换 在策略:TD目标值中的中的策略和采样的策略是同一个策略 离策略:TD目标值中的中的策略和采样的策略是不同的策略 一种特殊情况,TD目标值中的策略选择贪婪策略, 采样的策略选用ε-贪婪策略——Q学习 离策略评价Q学习 公式 算法流程 1:初始化且 2:repeat(对于每个片段) 3: 初始化状态 4: repeat(对于片段中每一步) 5: 根据选择一个在处的动作(使用-贪婪策略) 6: 执行动作，观测 7: 8: 9: until 是终止状态 10:until收敛 策略提升 -贪婪策略提升 " />
<meta name="twitter:site" content="http://localhost:1313/" />
<meta name="twitter:creator" content="" />
<meta name="twitter:image"
  content="http://localhost:1313/">


<meta property="og:type" content="article" />
<meta property="og:title" content="Temporal Difference - My New Hugo Site">
<meta property="og:description"
  content="Temporal Difference 增量式MC 时间差分TD 核心差别： MC是根据[s1→终止状态]完整片段的最终回报更新s1的值函数 TD是根据[s1→s2]这一步片段的即时回报值R和s2的估计值函数更新s1的值函数 与DP的对比 DP是全宽备份 TD是样本备份 TD与MC的对比 TD 算法在知道结果之前学习 TD算法在每一步之后都能在线学习 MC算法必须等待最终回报值得到之后才能学习 TD算法即便没有最终结果也能学习 TD算法能够从不完整序列中学习 MC算法仅仅能够从完整序列中学习 TD算法适用于连续性任务和片段性任务 MC算法仅仅适用于片段性任务 TD算法有多个驱动力 MC算法只有奖励值作为更新的驱动力 TD算法有奖励值和状态转移作为更新的驱动力 MC有高方差,零偏差 收敛性较好 (即使采用函数逼近) 对初始值不太敏感 随着样本数量的增加,方差逐渐减少, 趋近于 0 TD 有低方差,和一些偏差 通常比 MC 效率更高 表格法下TD(0)收敛到(函数逼近时不一定) 对初始值更敏感 随着样本数量的增加,偏差逐渐减少,趋近于 0 样本数量有限时，TD的结果与真实结果的偏差比较稳定。MC可能出现巨大偏差。 TD要求环境符合马尔科夫性，MC不要求 自举和采样 自举: 使用随机变量的估计去更新 MC 没有自举 DP 和 TD 都有自举 采样: 通过样本估计期望 MC 和 TD 采样 DP 不采样 TD的优化方法 整体思路是 策略评价&#43;策略提升 策略评价 在策略评价SARSA 公式 算法流程 1:初始化且 2:repeat(对于每个片段) 3: 初始化状态 4: 根据选择一个在处的动作(使用-贪婪策略) 5: repeat(对于片段中每一步) 6: 执行动作，观测 7: 根据选择一个在处的动作(使用-贪婪策略) 8: 9: 10: until 是终止状态 11:until收敛 收敛性 在满足以下条件时,Sarsa 算法收敛到最优的状态动作值函数 策略序列满足GLIE 步长序列是一个Robbins-Monro序列 GLIE 保证了 充分的探索 策略最终收敛到贪婪的策略 Robbins-Monro保证了 步长足够大,足以克服任意初始值 步长足够小,最终收敛 (常量步长不满足) 期望SARSA 减少了由于的选择带来的方差 在相同更新步数时,期望 Sarsa 比 Sarsa 的通用性更好 可以在在策略和离策略中切换 在策略:TD目标值中的中的策略和采样的策略是同一个策略 离策略:TD目标值中的中的策略和采样的策略是不同的策略 一种特殊情况,TD目标值中的策略选择贪婪策略, 采样的策略选用ε-贪婪策略——Q学习 离策略评价Q学习 公式 算法流程 1:初始化且 2:repeat(对于每个片段) 3: 初始化状态 4: repeat(对于片段中每一步) 5: 根据选择一个在处的动作(使用-贪婪策略) 6: 执行动作，观测 7: 8: 9: until 是终止状态 10:until收敛 策略提升 -贪婪策略提升 " />
<meta property="og:url" content="http://localhost:1313/post/reinforcement-learning/temporal-difference/" />
<meta property="og:site_name" content="Temporal Difference" />
<meta property="og:image"
  content="http://localhost:1313/">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">











</head>


<body>
  <div style="position: relative">
  <header class="Header js-details-container Details px-3 px-md-4 px-lg-5 flex-wrap flex-md-nowrap open Details--on">
    <div class="Header-item mobile-none" style="margin-top: -4px; margin-bottom: -4px;">
      <a class="Header-link" href="http://localhost:1313/">
        <img class="octicon" height="32" width="32" src="/images/github-mark-white.png">
      </a>
    </div>
    <div class="Header-item d-md-none">
      <button class="Header-link btn-link js-details-target" type="button"
        onclick="document.querySelector('#header-search').style.display = document.querySelector('#header-search').style.display == 'none'? 'block': 'none'">
        <svg height="24" class="octicon octicon-three-bars" viewBox="0 0 16 16" version="1.1" width="24">
          <path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z">
          </path>
        </svg>
      </button>
    </div>
    <div style="display: none;" id="header-search"
      class="Header-item Header-item--full flex-column flex-md-row width-full flex-order-2 flex-md-order-none mr-0 mr-md-3 mt-3 mt-md-0 Details-content--hidden-not-important d-md-flex">
      <div
        class="Header-search header-search flex-auto js-site-search position-relative flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to">
        <div class="position-relative">
          
          <form target="_blank" action="https://www.google.com/search" accept-charset="UTF-8" method="get"
            autocomplete="off">
            <label
              class="Header-search-label form-control input-sm header-search-wrapper p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center">
              <input type="text"
                class="Header-search-input form-control input-sm header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable"
                name="q" value="" placeholder="Search" autocomplete="off">
              <input type="hidden" name="q" value="site:http://localhost:1313/">
            </label>
          </form>
          
        </div>
      </div>
    </div>

    <div class="Header-item Header-item--full flex-justify-center d-md-none position-relative">
      <a class="Header-link " href="http://localhost:1313/">
        <img class="octicon octicon-mark-github v-align-middle" height="32" width="32" src="/images/github-mark-white.png">
      </a>
    </div>
    <div class="Header-item" style="margin-right: 0;">
      <a href="javascript:void(0)" class="Header-link no-select" onclick="switchTheme()">
        <svg style="fill: var(--color-profile-color-modes-toggle-moon);" class="no-select" viewBox="0 0 16 16"
          version="1.1" width="16" height="16">
          <path fill-rule="evenodd" clip-rule="evenodd"
            d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z">
          </path>
        </svg>
      </a>
    </div>
  </header>
</div>

  <div id="search-result" class="container-lg px-3 new-discussion-timeline" style="display: none;">
</div>

  
<div class="application-main">
  <div>
  <main>
    <div class="gisthead pagehead bg-gray-light pb-0 pt-3 mb-4">
      <div class="px-0">
        <div class="mb-3 d-flex px-3 px-md-3 px-lg-5">
          <div class="flex-auto min-width-0 width-fit mr-3">
            <div class="d-flex">
              <div class="d-none d-md-block">
                <a class="avatar mr-2 flex-shrink-0" href="http://localhost:1313/">
                  <img class=" avatar-user"
                    src="/images/avatar.png"
                    width="32" height="32"></a>
              </div>
              <div class="d-flex flex-column">
                <h1 class="break-word f3 text-normal mb-md-0 mb-1">
                  <span class="author">
                    <a href="http://localhost:1313/">looyifan</a>
                  </span>
                  <span class="path-divider">/</span>
                  <strong class="css-truncate css-truncate-target mr-1" style="max-width: 410px">
                    <a href="http://localhost:1313/post/reinforcement-learning/temporal-difference/">Temporal Difference</a>
                  </strong>
                </h1>
                <div class="note m-0">
                  Created <relative-time datetime="Mon, 01 Jan 0001 00:00:00 &#43;0000"
                    class="no-wrap">
                    Mon, 01 Jan 0001 00:00:00 &#43;0000</relative-time>

                  
                  <span class="file-info-divider"></span>
                  Modified <relative-time datetime="Thu, 03 Apr 2025 17:04:04 &#43;0800"
                    class="no-wrap">
                    Thu, 03 Apr 2025 17:04:04 &#43;0800</relative-time>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container-lg px-3 new-discussion-timeline">
      <div class="repository-content gist-content">
        <div>
          <div class="js-gist-file-update-container js-task-list-container file-box">
            <div id="file-pytest" class="file my-2">
              <div id="post-header" class="file-header d-flex flex-md-items-center flex-items-start sticky-header" style="z-index: 2">
                <div class="file-info d-flex flex-md-items-center flex-items-start flex-order-1 flex-auto">
                  <div class="text-mono f6 flex-auto pr-3 flex-order-2 flex-md-order-1 mt-2 mt-md-0">
                    
                    <summary id="toc-toggle" onclick="clickToc()" class="btn btn-octicon m-0 mr-2 p-2">
                      <svg aria-hidden="true" viewBox="0 0 16 16" height="16" width="16" class="octicon octicon-list-unordered">
                        <path fill-rule="evenodd" d="M2 4a1 1 0 100-2 1 1 0 000 2zm3.75-1.5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zm0 5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zm0 5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zM3 8a1 1 0 11-2 0 1 1 0 012 0zm-1 6a1 1 0 100-2 1 1 0 000 2z"></path>
                      </svg>
                    </summary>
                    <details-menu class="SelectMenu" id="toc-details" style="display: none;">
                      <div class="SelectMenu-modal rounded-3 mt-1" style="max-height: 340px;">
                        <div class="SelectMenu-list SelectMenu-list--borderless p-2" style="overscroll-behavior: contain;" id="toc-list">
                        </div>
                      </div>
                    </details-menu>
                      1081 Words
                    

                  </div>
                  <div class="file-actions flex-order-2 pt-0">
                    
                  </div>
                </div>
              </div>


              <div class="Box-body px-5 pb-5" style="z-index: 1">
                <article class="markdown-body entry-content container-lg"><h1 id="temporal-difference">Temporal Difference</h1>
<ul>
<li>增量式MC
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>时间差分TD
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>核心差别：
<ul>
<li>MC是根据[s1→终止状态]完整片段的最终回报更新s1的值函数</li>
<li>TD是根据[s1→s2]这一步片段的即时回报值R和s2的估计值函数更新s1的值函数</li>
</ul>
</li>
</ul>
<h1 id="与dp的对比">与DP的对比</h1>
<ul>
<li>DP是全宽备份</li>
<li>TD是样本备份</li>
</ul>
<h1 id="td与mc的对比">TD与MC的对比</h1>
<ul>
<li><strong>TD 算法在知道结果之前学习</strong>
<ul>
<li>TD算法在每一步之后都能在线学习</li>
<li>MC算法必须等待最终回报值得到之后才能学习</li>
</ul>
</li>
<li><strong>TD算法即便没有最终结果也能学习</strong>
<ul>
<li>TD算法能够从不完整序列中学习</li>
<li>MC算法仅仅能够从完整序列中学习</li>
<li>TD算法适用于连续性任务和片段性任务</li>
<li>MC算法仅仅适用于片段性任务</li>
</ul>
</li>
<li><strong>TD算法有多个驱动力</strong>
<ul>
<li>MC算法只有奖励值作为更新的驱动力</li>
<li>TD算法有奖励值和状态转移作为更新的驱动力</li>
</ul>
</li>
<li><strong>MC有高方差,零偏差</strong>
<ul>
<li>收敛性较好 (即使采用函数逼近)</li>
<li>对初始值不太敏感</li>
<li>随着样本数量的增加,方差逐渐减少, 趋近于 0</li>
</ul>
</li>
<li><strong>TD 有低方差,和一些偏差</strong>
<ul>
<li>通常比 MC 效率更高</li>
<li>表格法下TD(0)收敛到<!-- raw HTML omitted -->(函数逼近时不一定)</li>
<li>对初始值更敏感</li>
<li>随着样本数量的增加,偏差逐渐减少,趋近于 0</li>
<li>样本数量有限时，TD的结果与真实结果的偏差比较稳定。MC可能出现巨大偏差。</li>
</ul>
</li>
<li><strong>TD要求环境符合马尔科夫性，MC不要求</strong></li>
</ul>
<h1 id="自举和采样">自举和采样</h1>
<ul>
<li><strong>自举</strong>: 使用随机变量的估计去更新
<ul>
<li>MC 没有自举</li>
<li>DP 和 TD 都有自举</li>
</ul>
</li>
<li><strong>采样</strong>: 通过样本估计期望
<ul>
<li>MC 和 TD 采样</li>
<li>DP 不采样</li>
</ul>
</li>
</ul>
<h1 id="td的优化方法">TD的优化方法</h1>
<ul>
<li>整体思路是 策略评价+策略提升</li>
<li>
<h2 id="策略评价">策略评价</h2>
<ul>
<li>
<h3 id="在策略评价sarsa">在策略评价<strong>SARSA</strong></h3>
<ul>
<li>
<h4 id="公式">公式</h4>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h4 id="算法流程">算法流程</h4>
<ul>
<li>1:初始化<!-- raw HTML omitted -->且<!-- raw HTML omitted --></li>
<li>2:repeat(对于每个片段)</li>
<li>3:  初始化状态<!-- raw HTML omitted --></li>
<li>4:  根据<!-- raw HTML omitted -->选择一个在<!-- raw HTML omitted -->处的动作<!-- raw HTML omitted -->(使用<!-- raw HTML omitted -->-贪婪策略)</li>
<li>5:  repeat(对于片段中每一步)</li>
<li>6:    执行动作<!-- raw HTML omitted -->，观测<!-- raw HTML omitted --></li>
<li>7:    根据<!-- raw HTML omitted -->选择一个在<!-- raw HTML omitted -->处的动作<!-- raw HTML omitted -->(使用<!-- raw HTML omitted -->-贪婪策略)</li>
<li>8:    <!-- raw HTML omitted --></li>
<li>9:    <!-- raw HTML omitted --></li>
<li>10:  until <!-- raw HTML omitted -->是终止状态</li>
<li>11:until收敛</li>
</ul>
</li>
<li>
<h4 id="收敛性">收敛性</h4>
<ul>
<li>在满足以下条件时,Sarsa 算法收敛到最优的状态动作值函数
<ul>
<li>策略序列<!-- raw HTML omitted -->满足GLIE</li>
<li>步长序列<!-- raw HTML omitted -->是一个Robbins-Monro序列
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
</ul>
</li>
<li>GLIE 保证了
<ul>
<li>充分的探索</li>
<li>策略最终收敛到贪婪的策略</li>
</ul>
</li>
<li>Robbins-Monro保证了
<ul>
<li>步长足够大,足以克服任意初始值</li>
<li>步长足够小,最终收敛 (常量步长不满足)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="期望sarsa">期望SARSA</h3>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
<li>减少了由于<!-- raw HTML omitted -->的选择带来的方差</li>
<li>在相同更新步数时,期望 Sarsa 比 Sarsa 的通用性更好</li>
<li>可以在在策略和离策略中切换
<ul>
<li>在策略:TD目标值中的<!-- raw HTML omitted -->中的策略<!-- raw HTML omitted -->和采样的策略是同一个策略</li>
<li>离策略:TD目标值中的<!-- raw HTML omitted -->中的策略<!-- raw HTML omitted -->和采样的策略是不同的策略</li>
</ul>
</li>
<li>一种特殊情况,TD目标值中的策略选择贪婪策略, 采样的策略选用ε-贪婪策略——<strong>Q学习</strong></li>
</ul>
</li>
<li>
<h3 id="离策略评价q学习">离策略评价<strong>Q学习</strong></h3>
<ul>
<li>
<h4 id="公式-1">公式</h4>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h4 id="算法流程-1">算法流程</h4>
<ul>
<li>1:初始化<!-- raw HTML omitted -->且<!-- raw HTML omitted --></li>
<li>2:repeat(对于每个片段)</li>
<li>3:  初始化状态<!-- raw HTML omitted --></li>
<li>4:  repeat(对于片段中每一步)</li>
<li>5:    根据<!-- raw HTML omitted -->选择一个在<!-- raw HTML omitted -->处的动作<!-- raw HTML omitted -->(使用<!-- raw HTML omitted -->-贪婪策略)</li>
<li>6:    执行动作<!-- raw HTML omitted -->，观测<!-- raw HTML omitted --></li>
<li>7:    <!-- raw HTML omitted --></li>
<li>8:    <!-- raw HTML omitted --></li>
<li>9:  until <!-- raw HTML omitted -->是终止状态</li>
<li>10:until收敛</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="策略提升">策略提升</h2>
<ul>
<li>
<h2 id="-贪婪策略提升"><!-- raw HTML omitted -->-贪婪策略提升</h2>
</li>
</ul>
</li>
</ul>
</article>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </main>
</div>
<script type="application/javascript" src='http://localhost:1313/js/toc.js'></script>
<link rel="stylesheet" href='http://localhost:1313/css/toc.css' />

  
<div id="gitalk-container" class="gitalk-container"></div>
<link rel="stylesheet" href='http://localhost:1313/css/gitalk.css'>
<script src='http://localhost:1313/js/gitalk.min.js'></script>
<script>
  const gitalk = new Gitalk({
    clientID: '',
    clientSecret: '',
    repo: '',
    owner: '',
    admin: [''],
    id: eval( null ), 
    distractionFreeMode: false 
  });
  (function() {
    gitalk.render('gitalk-container');
  })();
</script>

</div>

  <div class="footer container-xl width-full p-responsive">
  <div
    class="position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between flex-sm-items-center pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light ">
    <a aria-label="Homepage" title="GitHub" class="footer-octicon d-none d-lg-block mr-lg-4" href="http://localhost:1313/">
      <svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="24">
        <path fill-rule="evenodd"
          d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z">
        </path>
      </svg>
    </a>
    <ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0">
      
      <li class="mr-3 mr-lg-0">Theme by <a href='https://github.com/MeiK2333/github-style'>github-style</a></li>
      
      <li class="mr-3 mr-lg-0">GitHub and the Invertocat logo are trademarks of <a href="https://github.com/">GitHub, Inc.</a></li>
    </ul>
  </div>
  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 text-gray-light"></span>
  </div>


</div>

</body>

<script type="application/javascript" src="http://localhost:1313/js/github-style.js"></script>







</html>