<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <script type="application/javascript" src='http://localhost:1313/js/theme-mode.js'></script>
    <link rel="stylesheet" href='http://localhost:1313/css/frameworks.min.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/github.min.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/github-style.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/light.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/dark.css' />
    <link rel="stylesheet" href='http://localhost:1313/css/syntax.css' />
    <title>Reinforcement-Learning - My New Hugo Site</title>
    
    <link rel="icon" type="image/x-icon" href='/images/github.png'>
    
    <meta name="theme-color" content="#1e2327">

    <link rel="stylesheet" href="http://localhost:1313/css/custom.css">
    <script type="text/javascript" src="http://localhost:1313/js/custom.js"></script>

    
    <meta name="description"
  content="" />
<meta name="keywords"
  content='blog, robotics' />
<meta name="robots" content="noodp" />
<link rel="canonical" href="http://localhost:1313/categories/reinforcement-learning/" />


<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Reinforcement-Learning - My New Hugo Site" />
<meta name="twitter:description"
  content="" />
<meta name="twitter:site" content="http://localhost:1313/" />
<meta name="twitter:creator" content="" />
<meta name="twitter:image"
  content="http://localhost:1313/">


<meta property="og:type" content="website" />
<meta property="og:title" content="Reinforcement-Learning - My New Hugo Site">
<meta property="og:description"
  content="" />
<meta property="og:url" content="http://localhost:1313/categories/reinforcement-learning/" />
<meta property="og:site_name" content="Reinforcement-Learning" />
<meta property="og:image"
  content="http://localhost:1313/">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">




<link href="/categories/reinforcement-learning/index.xml" rel="alternate" type="application/rss+xml" title="My New Hugo Site" />








</head>


<body>
  <div style="position: relative">
  <header class="Header js-details-container Details px-3 px-md-4 px-lg-5 flex-wrap flex-md-nowrap open Details--on">
    <div class="Header-item mobile-none" style="margin-top: -4px; margin-bottom: -4px;">
      <a class="Header-link" href="http://localhost:1313/">
        <img class="octicon" height="32" width="32" src="/images/github-mark-white.png">
      </a>
    </div>
    <div class="Header-item d-md-none">
      <button class="Header-link btn-link js-details-target" type="button"
        onclick="document.querySelector('#header-search').style.display = document.querySelector('#header-search').style.display == 'none'? 'block': 'none'">
        <svg height="24" class="octicon octicon-three-bars" viewBox="0 0 16 16" version="1.1" width="24">
          <path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z">
          </path>
        </svg>
      </button>
    </div>
    <div style="display: none;" id="header-search"
      class="Header-item Header-item--full flex-column flex-md-row width-full flex-order-2 flex-md-order-none mr-0 mr-md-3 mt-3 mt-md-0 Details-content--hidden-not-important d-md-flex">
      <div
        class="Header-search header-search flex-auto js-site-search position-relative flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to">
        <div class="position-relative">
          
          <form target="_blank" action="https://www.google.com/search" accept-charset="UTF-8" method="get"
            autocomplete="off">
            <label
              class="Header-search-label form-control input-sm header-search-wrapper p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center">
              <input type="text"
                class="Header-search-input form-control input-sm header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable"
                name="q" value="" placeholder="Search" autocomplete="off">
              <input type="hidden" name="q" value="site:http://localhost:1313/">
            </label>
          </form>
          
        </div>
      </div>
    </div>

    <div class="Header-item Header-item--full flex-justify-center d-md-none position-relative">
      <a class="Header-link " href="http://localhost:1313/">
        <img class="octicon octicon-mark-github v-align-middle" height="32" width="32" src="/images/github-mark-white.png">
      </a>
    </div>
    <div class="Header-item" style="margin-right: 0;">
      <a href="javascript:void(0)" class="Header-link no-select" onclick="switchTheme()">
        <svg style="fill: var(--color-profile-color-modes-toggle-moon);" class="no-select" viewBox="0 0 16 16"
          version="1.1" width="16" height="16">
          <path fill-rule="evenodd" clip-rule="evenodd"
            d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z">
          </path>
        </svg>
      </a>
    </div>
  </header>
</div>

  <div id="search-result" class="container-lg px-3 new-discussion-timeline" style="display: none;">
</div>

  
<div class="application-main">
  


<div class="mt-4 position-sticky top-0 d-none d-md-block bg-white width-full border-bottom color-border-secondary"
  style="z-index:3;">
  <div class="container-xl px-3 px-md-4 px-lg-5">
    <div class="gutter-condensed gutter-lg flex-column flex-md-row d-flex">
      <div class="flex-shrink-0 col-12 col-md-3 mb-4 mb-md-0">
      </div>
      <div class="flex-shrink-0 col-12 col-md-9 mb-4 mb-md-0">
        <div class="UnderlineNav width-full box-shadow-none hx_UnderlineNav-with-profile-color-modes-banner">
          <nav class="UnderlineNav-body">
            <a class="UnderlineNav-item " href="http://localhost:1313/">
              <svg class="octicon octicon-book UnderlineNav-octicon hide-sm" height="16" viewBox="0 0 16 16"
                version="1.1" width="16">
                <path fill-rule="evenodd"
                  d="M0 1.75A.75.75 0 01.75 1h4.253c1.227 0 2.317.59 3 1.501A3.744 3.744 0 0111.006 1h4.245a.75.75 0 01.75.75v10.5a.75.75 0 01-.75.75h-4.507a2.25 2.25 0 00-1.591.659l-.622.621a.75.75 0 01-1.06 0l-.622-.621A2.25 2.25 0 005.258 13H.75a.75.75 0 01-.75-.75V1.75zm8.755 3a2.25 2.25 0 012.25-2.25H14.5v9h-3.757c-.71 0-1.4.201-1.992.572l.004-7.322zm-1.504 7.324l.004-5.073-.002-2.253A2.25 2.25 0 005.003 2.5H1.5v9h3.757a3.75 3.75 0 011.994.574z">
                </path>
              </svg>
              Overview
            </a>
            <a class="UnderlineNav-item " href="http://localhost:1313/post">
              <svg class="octicon octicon-repo UnderlineNav-octicon hide-sm" height="16" viewBox="0 0 16 16"
                version="1.1" width="16">
                <path fill-rule="evenodd"
                  d="M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z">
                </path>
              </svg>
              Posts
              <span class="Counter">96</span>
            </a>
          </nav>
          <div class="profile-color-modes js-promo-color-modes-banner-profile isInitialToggle">
            <svg width="106" height="60" viewBox="0 0 106 60" fill="none" stroke-width="3" stroke-linecap="round"
              stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg">
              <g class="profile-color-modes-illu-group profile-color-modes-illu-red">
                <path d="M37.5 58.5V57.5C37.5 49.768 43.768 43.5 51.5 43.5V43.5C59.232 43.5 65.5 49.768 65.5 57.5V58.5">
                </path>
              </g>
              <g class="profile-color-modes-illu-group profile-color-modes-illu-orange">
                <path
                  d="M104.07 58.5C103.401 55.092 97.7635 54.3869 95.5375 57.489C97.4039 54.6411 99.7685 48.8845 94.6889 46.6592C89.4817 44.378 86.1428 50.1604 85.3786 54.1158C85.9519 50.4768 83.7226 43.294 78.219 44.6737C72.7154 46.0534 72.7793 51.3754 74.4992 55.489C74.169 54.7601 72.4917 53.3567 70.5 52.8196">
                </path>
              </g>
              <g class="profile-color-modes-illu-group profile-color-modes-illu-purple">
                <path
                  d="M5.51109 58.5V52.5C5.51109 41.4543 14.4654 32.5 25.5111 32.5C31.4845 32.5 36.8464 35.1188 40.5111 39.2709C40.7212 39.5089 40.9258 39.7521 41.1245 40">
                </path>
                <path d="M27.511 49.5C29.6777 49.5 28.911 49.5 32.511 49.5"></path>
                <path d="M27.511 56.5C29.6776 56.5 26.911 56.5 30.511 56.5"></path>
              </g>
              <g class="profile-color-modes-illu-group profile-color-modes-illu-green">
                <circle cx="5.5" cy="12.5" r="4"></circle>
                <circle cx="18.5" cy="5.5" r="4"></circle>
                <path d="M18.5 9.5L18.5 27.5"></path>
                <path d="M18.5 23.5C6 23.5 5.5 23.6064 5.5 16.5"></path>
              </g>
              <g class="profile-color-modes-illu-group profile-color-modes-illu-blue">
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M40.6983 31.5C40.5387 29.6246 40.6456 28.0199 41.1762 27.2317C42.9939 24.5312 49.7417 26.6027 52.5428 30.2409C54.2551 29.8552 56.0796 29.6619 57.9731 29.6619C59.8169 29.6619 61.5953 29.8452 63.2682 30.211C66.0833 26.5913 72.799 24.5386 74.6117 27.2317C75.6839 28.8246 75.0259 33.7525 73.9345 37.5094C74.2013 37.9848 74.4422 38.4817 74.6555 39">
                  </path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M41.508 31.5C41.6336 31.2259 41.7672 30.9582 41.9085 30.6968C40.7845 26.9182 40.086 21.8512 41.1762 20.2317C42.9939 17.5312 49.7417 19.6027 52.5428 23.2409C54.2551 22.8552 56.0796 22.6619 57.9731 22.6619C59.8169 22.6619 61.5953 22.8452 63.2682 23.211C66.0833 19.5913 72.799 17.5386 74.6117 20.2317C75.6839 21.8246 75.0259 26.7525 73.9345 30.5094C75.1352 32.6488 75.811 35.2229 75.811 38.2283C75.811 38.49 75.8058 38.7472 75.7957 39">
                  </path>
                  <path d="M49.4996 33V35.6757"></path>
                  <path d="M67.3375 33V35.6757"></path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M41.508 31.5C41.6336 31.2259 41.7672 30.9582 41.9085 30.6968C40.7845 26.9182 40.086 21.8512 41.1762 20.2317C42.9939 17.5312 49.7417 19.6027 52.5428 23.2409C54.2551 22.8552 56.0796 22.6619 57.9731 22.6619C59.8169 22.6619 61.5953 22.8452 63.2682 23.211C66.0833 19.5913 72.799 17.5386 74.6117 20.2317C75.6839 21.8246 75.0259 26.7525 73.9345 30.5094C75.1352 32.6488 75.811 35.2229 75.811 38.2283C75.811 38.49 75.8058 38.7472 75.7957 39">
                  </path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M41.508 31.5C41.6336 31.2259 41.7672 30.9582 41.9085 30.6968C40.7845 26.9182 40.086 21.8512 41.1762 20.2317C42.9939 17.5312 49.7417 19.6027 52.5428 23.2409C54.2551 22.8552 56.0796 22.6619 57.9731 22.6619C59.8169 22.6619 61.5953 22.8452 63.2682 23.211C66.0833 19.5913 72.799 17.5386 74.6117 20.2317C75.6839 21.8246 75.0259 26.7525 73.9345 30.5094C75.1352 32.6488 75.811 35.2229 75.811 38.2283C75.811 38.49 75.8058 38.7472 75.7957 39">
                  </path>
                  <path d="M49.4996 33V35.6757"></path>
                  <path d="M67.3375 33V35.6757"></path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M41.508 31.5C41.6336 31.2259 41.7672 30.9582 41.9085 30.6968C40.7845 26.9182 40.086 21.8512 41.1762 20.2317C42.9939 17.5312 49.7417 19.6027 52.5428 23.2409C54.2551 22.8552 56.0796 22.6619 57.9731 22.6619C59.8169 22.6619 61.5953 22.8452 63.2682 23.211C66.0833 19.5913 72.799 17.5386 74.6117 20.2317C75.6839 21.8246 75.0259 26.7525 73.9345 30.5094C75.1352 32.6488 75.811 35.2229 75.811 38.2283C75.811 38.49 75.8058 38.7472 75.7957 39">
                  </path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M41.508 31.5C41.6336 31.2259 41.7672 30.9582 41.9085 30.6968C40.7845 26.9182 40.086 21.8512 41.1762 20.2317C42.9939 17.5312 49.7417 19.6027 52.5428 23.2409C54.2551 22.8552 56.0796 22.6619 57.9731 22.6619C59.8169 22.6619 61.5953 22.8452 63.2682 23.211C66.0833 19.5913 72.799 17.5386 74.6117 20.2317C75.6839 21.8246 75.0259 26.7525 73.9345 30.5094C75.1352 32.6488 75.811 35.2229 75.811 38.2283C75.811 38.49 75.8058 38.7472 75.7957 39">
                  </path>
                  <path d="M49.4996 33V35.6757"></path>
                  <path d="M67.3375 33V35.6757"></path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M73.4999 40.2236C74.9709 38.2049 75.8108 35.5791 75.8108 32.2283C75.8108 29.2229 75.1351 26.6488 73.9344 24.5094C75.0258 20.7525 75.6838 15.8246 74.6116 14.2317C72.7989 11.5386 66.0832 13.5913 63.2681 17.211C61.5952 16.8452 59.8167 16.6619 57.973 16.6619C56.0795 16.6619 54.2549 16.8552 52.5427 17.2409C49.7416 13.6027 42.9938 11.5312 41.176 14.2317C40.0859 15.8512 40.7843 20.9182 41.9084 24.6968C41.003 26.3716 40.4146 28.3065 40.2129 30.5">
                  </path>
                  <path d="M82.9458 30.5471L76.8413 31.657"></path>
                  <path d="M76.2867 34.4319L81.8362 37.7616"></path>
                  <path d="M49.4995 27.8242V30.4999"></path>
                  <path d="M67.3374 27.8242V30.4998"></path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M45.3697 34.2658C41.8877 32.1376 39.7113 28.6222 39.7113 23.2283C39.7113 20.3101 40.3483 17.7986 41.4845 15.6968C40.3605 11.9182 39.662 6.85125 40.7522 5.23168C42.5699 2.53117 49.3177 4.6027 52.1188 8.24095C53.831 7.85521 55.6556 7.66186 57.5491 7.66186C59.3929 7.66186 61.1713 7.84519 62.8442 8.21095C65.6593 4.59134 72.375 2.5386 74.1877 5.23168C75.2599 6.82461 74.6019 11.7525 73.5105 15.5094C74.7112 17.6488 75.3869 20.2229 75.3869 23.2283C75.3869 28.6222 73.2105 32.1376 69.7285 34.2658C70.8603 35.5363 72.6057 38.3556 73.3076 40">
                  </path>
                  <path d="M49.0747 19.8242V22.4999"></path>
                  <path
                    d="M54.0991 28C54.6651 29.0893 55.7863 30.0812 57.9929 30.0812C59.0642 30.0812 59.8797 29.8461 60.5 29.4788">
                  </path>
                  <path d="M66.9126 19.8242V22.4999"></path>
                  <path d="M33.2533 20.0237L39.0723 22.1767"></path>
                  <path d="M39.1369 25.0058L33.0935 27.3212"></path>
                  <path d="M81.8442 19.022L76.0252 21.1751"></path>
                  <path d="M75.961 24.0041L82.0045 26.3196"></path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M73.4999 40.2236C74.9709 38.2049 75.8108 35.5791 75.8108 32.2283C75.8108 29.2229 75.1351 26.6488 73.9344 24.5094C75.0258 20.7525 75.6838 15.8246 74.6116 14.2317C72.7989 11.5386 66.0832 13.5913 63.2681 17.211C61.5952 16.8452 59.8167 16.6619 57.973 16.6619C56.0795 16.6619 54.2549 16.8552 52.5427 17.2409C49.7416 13.6027 42.9938 11.5312 41.176 14.2317C40.0859 15.8512 40.7843 20.9182 41.9084 24.6968C41.003 26.3716 40.4146 28.3065 40.2129 30.5">
                  </path>
                  <path d="M82.9458 30.5471L76.8413 31.657"></path>
                  <path d="M76.2867 34.4319L81.8362 37.7616"></path>
                  <path d="M49.4995 27.8242V30.4999"></path>
                  <path d="M67.3374 27.8242V30.4998"></path>
                </g>
                <g class="profile-color-modes-illu-frame">
                  <path
                    d="M40.6983 31.5C40.5387 29.6246 40.6456 28.0199 41.1762 27.2317C42.9939 24.5312 49.7417 26.6027 52.5428 30.2409C54.2551 29.8552 56.0796 29.6619 57.9731 29.6619C59.8169 29.6619 61.5953 29.8452 63.2682 30.211C66.0833 26.5913 72.799 24.5386 74.6117 27.2317C75.6839 28.8246 75.0259 33.7525 73.9345 37.5094C74.2013 37.9848 74.4422 38.4817 74.6555 39">
                  </path>
                </g>
              </g>
            </svg>
            <span class="profile-color-modes-toggle js-promo-color-modes-toggle no-select" tabindex="0"
              onclick="switchTheme()">
              <div class="profile-color-modes-toggle-track no-select"></div>
              <div class="profile-color-modes-toggle-thumb js-promo-color-modes-thumb">
                <svg style="fill: var(--color-profile-color-modes-toggle-moon); margin: 7px 0 0 7px;" width="14"
                  height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg">
                  <path fill-rule="evenodd" clip-rule="evenodd"
                    d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z">
                  </path>
                </svg>
              </div>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container-xl px-3 px-md-4 px-lg-5">
  <div class="gutter-condensed gutter-lg flex-column flex-md-row d-flex">
    <div class="flex-shrink-0 col-12 col-md-3 mb-4 mb-md-0">
      <div class="h-card mt-md-n5" style="margin-top:24px">
        <div class="user-profile-sticky-bar js-user-profile-sticky-bar d-none d-md-block" id="headerStuck">
          <div class="user-profile-mini-vcard d-table">
            <span class="user-profile-mini-avatar d-table-cell v-align-middle lh-condensed-ultra pr-2">
              
              <img class="rounded-1 avatar-user" height="32" width="32" src="/images/avatar.png">
              
            </span>
            <span class="d-table-cell v-align-middle lh-condensed">
              <strong>looyifan</strong>
            </span>
          </div>
        </div>
        <div class="clearfix d-flex d-md-block flex-items-center mb-4 mb-md-0">
          <div class="position-relative d-inline-block col-2 col-md-12 mr-3 mr-md-0 flex-shrink-0" style="z-index:4;">
            
            <a href="/images/avatar.png">
              <img style="height:auto;" alt="Avatar" width="260" height="260" id="headerImg"
                class="avatar avatar-user width-full border bg-white" src="/images/avatar.png">
            </a>
            
            
            <div class="user-status-container position-relative hide-sm hide-md">
              <div class="f5 user-status-circle-badge-container">
                <div class="user-status-circle-badge d-inline-block lh-condensed-ultra p-2">
                  <div class="d-flex flex-items-center flex-items-stretch">
                    <div class="f6 lh-condensed user-status-header d-inline-flex user-status-emoji-only-header circle">
                      <div class="user-status-emoji-container flex-shrink-0 mr-2 d-flex flex-items-center flex-justify-center ">
                        <div><g-emoji class="g-emoji">😀</g-emoji></div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>

          <div
            class="vcard-names-container float-left col-10 col-md-12 pt-1 pt-md-3 pb-1 pb-md-3 js-sticky js-user-profile-sticky-fields"
            data-original-top="0px" style="position: sticky;">
            <h1 class="vcard-names pl-2 pl-md-0">
              <span class="p-name vcard-fullname d-block overflow-hidden">looyifan</span>
              
              <span class="p-nickname vcard-username d-block">xwlu</span>
              
            </h1>
          </div>
        </div>

        <div class="p-note user-profile-bio mb-3 js-user-profile-bio f4">
          <div>日拱一卒</div>
        </div>

        <div class="d-flex flex-column">
          <div class="js-profile-editable-area d-flex flex-column d-md-block">
            <ul class="vcard-details">
              
              <li class="vcard-detail pt-1 css-truncate css-truncate-target hide-sm hide-md">
                <svg class="octicon octicon-location" viewBox="0 0 16 16" version="1.1" width="16" height="16">
                  <path fill-rule="evenodd"
                    d="M11.536 3.464a5 5 0 010 7.072L8 14.07l-3.536-3.535a5 5 0 117.072-7.072v.001zm1.06 8.132a6.5 6.5 0 10-9.192 0l3.535 3.536a1.5 1.5 0 002.122 0l3.535-3.536zM8 9a2 2 0 100-4 2 2 0 000 4z">
                  </path>
                </svg>
                <span class="p-label">China</span>
              </li>
              

              
              <li class="vcard-detail pt-1 css-truncate css-truncate-target ">
                <svg class="octicon octicon-mail" viewBox="0 0 16 16" version="1.1" width="16" height="16">
                  <path fill-rule="evenodd"
                    d="M1.75 2A1.75 1.75 0 000 3.75v.736a.75.75 0 000 .027v7.737C0 13.216.784 14 1.75 14h12.5A1.75 1.75 0 0016 12.25v-8.5A1.75 1.75 0 0014.25 2H1.75zM14.5 4.07v-.32a.25.25 0 00-.25-.25H1.75a.25.25 0 00-.25.25v.32L8 7.88l6.5-3.81zm-13 1.74v6.441c0 .138.112.25.25.25h12.5a.25.25 0 00.25-.25V5.809L8.38 9.397a.75.75 0 01-.76 0L1.5 5.809z">
                  </path>
                </svg>
                <a class="u-email link-gray-dark " href="mailto:looyifan@gmail.com">looyifan@gmail.com</a>
              </li>
              

              <li class="vcard-detail pt-1 css-truncate css-truncate-target ">
                <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16">
                  <path fill-rule="evenodd"
                    d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
                  </path>
                </svg>
                
                <a rel="nofollow me" class="link-gray-dark" href="https://xwlu.github.io">https://xwlu.github.io</a>
                
              </li>
            </ul>
          </div>
        </div>

        <div class="border-top color-border-secondary pt-3 mt-3 clearfix hide-sm hide-md">
          <h2 class="mb-2 h4">Organizations</h2>
          <div style="display:flex;justify-content:flex-start;flex-wrap:wrap;margin-bottom:3px;">
          
          <a style="margin: 0 10px 10px 0;" href="https://github.com/xwlu">
            <svg id="github-icon" viewBox="0 0 16 16" version="1.1" width="32" height="32" fill="#24292e">
              <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z">
              </path>
            </svg>
          </a>
          

          
          <a style="margin: 0 10px 10px 0;" href="https://twitter.com/looyifan">
            <svg  width="32" height="32" viewBox="0 0 1200 1227" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path id="twitter-icon" d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z" fill="white"/>
            </svg>
          </a>
          

          

          

          

          

          

          

          
          
          
          <a style="margin: 0 10px 10px 0;" href="https://xwlu.github.io">
            <img alt="@Link" width="32" height="32" src='http://localhost:1313/images/link.png' class="avatar">
          </a>
          
          
          
          <a style="margin: 0 10px 10px 0;" href="https://xwlu.github.io">
            <img alt="@Link2" width="32" height="32" src="https://xwlu.github.io/images/avatar.png" class="avatar">
          </a>
          
          
          

          
          <a style="margin: 0 10px 10px 0;" href="http://localhost:1313/index.xml">
            <img alt="@rss" width="32" height="32" src="http://localhost:1313/images/rss.png" class="avatar">
          </a>
          
         </div>
        </div>
      </div>
    </div>

    <div class="flex-shrink-0 col-12 col-md-9 mb-4 mb-md-0">
      

<div class="UnderlineNav user-profile-nav d-block d-md-none position-sticky top-0 pl-3 ml-n3 mr-n3 pr-3 bg-white"
  style="z-index:3;">
  <nav class="UnderlineNav-body">
    <a class="UnderlineNav-item " href="http://localhost:1313/">
      <svg class="octicon octicon-book UnderlineNav-octicon hide-sm" height="16" viewBox="0 0 16 16" version="1.1"
        width="16">
        <path fill-rule="evenodd"
          d="M0 1.75A.75.75 0 01.75 1h4.253c1.227 0 2.317.59 3 1.501A3.744 3.744 0 0111.006 1h4.245a.75.75 0 01.75.75v10.5a.75.75 0 01-.75.75h-4.507a2.25 2.25 0 00-1.591.659l-.622.621a.75.75 0 01-1.06 0l-.622-.621A2.25 2.25 0 005.258 13H.75a.75.75 0 01-.75-.75V1.75zm8.755 3a2.25 2.25 0 012.25-2.25H14.5v9h-3.757c-.71 0-1.4.201-1.992.572l.004-7.322zm-1.504 7.324l.004-5.073-.002-2.253A2.25 2.25 0 005.003 2.5H1.5v9h3.757a3.75 3.75 0 011.994.574z">
        </path>
      </svg>
      Overview
    </a>
    <a class='UnderlineNav-item  '
      href="http://localhost:1313/post">
      <svg class="octicon octicon-repo UnderlineNav-octicon hide-sm" height="16" viewBox="0 0 16 16" version="1.1"
        width="16">
        <path fill-rule="evenodd"
          d="M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z">
        </path>
      </svg>
      Posts
      <span class="Counter ">96</span>
    </a>
  </nav>
</div>

      
      
<div>
  <div class="position-relative">
    <div>
      
        
      
      <ul>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/common/">Common Knowledges on Reinforcement Learning</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="问题定义">问题定义</h1>
<pre tabindex="0"><code>每个时刻&lt;img src=&#34;https://latex.codecogs.com/svg.image?t&#34;/&gt;
- 智能体(Agent)
  - 智能体执行动作&lt;img src=&#34;https://latex.codecogs.com/svg.image?A_{t}&#34;/&gt;，并在环境中得到观测&lt;img src=&#34;https://latex.codecogs.com/svg.image?O_{t}&#34;/&gt;和奖励&lt;img src=&#34;https://latex.codecogs.com/svg.image?R_{t}&#34;/&gt;
- 环境(Environment)
  - 环境会对智能体的动作&lt;img src=&#34;https://latex.codecogs.com/svg.image?A_{t}&#34;/&gt;的做出反应,然后发送新的观测&lt;img src=&#34;https://latex.codecogs.com/svg.image?O_{t+1}&#34;/&gt;和奖励&lt;img src=&#34;https://latex.codecogs.com/svg.image?R_{t+1}&#34;/&gt;
</code></pre><h1 id="核心概念">核心概念</h1>
<ul>
<li>
<h2 id="智能体">智能体</h2>
<ul>
<li>智能体是指强化学习需要优化的部分,是我们能够精确控制的部分</li>
</ul>
</li>
<li>
<h2 id="环境">环境</h2>
<ul>
<li>环境是我们不能直接控制的部分</li>
<li>环境并不是指自然环境，不同的问题,智能体和环境的划分也有所区别
<ul>
<li>机器人探索房间 vs. 机器人行走控制</li>
<li>仿真环境中的控制 vs. 实际环境中的控制</li>
</ul>
</li>
<li>区分智能体和环境是强化学习的第一步</li>
</ul>
</li>
<li>
<h2 id="奖励reward">奖励(Reward)</h2>
<ul>
<li>奖励是强化学习的核心</li>
<li>可以没有观测,但是不能没有奖励</li>
<li>奖励是强化学习区别其他机器学习的标志特征</li>
<li>特点
<ul>
<li>奖励<!-- raw HTML omitted -->是一个<strong>标量</strong>反馈</li>
<li>它衡量了智能体在时间<!-- raw HTML omitted -->上做得有多好</li>
<li>智能体的目标就是<strong>最大化累计奖励</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="强化学习组成">强化学习组成</h1>
<ul>
<li>
<h2 id="奖励">奖励</h2>
</li>
</ul>
<blockquote>
<p>指智能体在执行某个动作<!-- raw HTML omitted -->后得到的累计回报<!-- raw HTML omitted -->，<!-- raw HTML omitted -->，<!-- raw HTML omitted -->。其中<!-- raw HTML omitted -->越小表示我们越关注短期奖励，<!-- raw HTML omitted -->越大表示我们越关注长期奖励</p>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/dqn/">Deep Q Learning</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="基于值函数的深度学习网络">基于值函数的深度学习网络</h1>
<ul>
<li>
<h2 id="dqn">DQN</h2>
<ul>
<li>深度强化学习的鼻祖式工作</li>
<li>
<h3 id="解决了两个问题">解决了两个问题</h3>
<ul>
<li>一段时间内的训练数据具有较强的相关性，如果按照顺序进行训练，会对当前状态下的情况产生过拟合。通过将历史训练数据存在一个buffer里，通过随机抽取的方式进行训练，解决了这一问题。</li>
<li>首次证明了能够通过raw pixels解决游戏问题，对所有游戏通用</li>
</ul>
</li>
<li>
<h3 id="关键特点">关键特点</h3>
<ul>
<li>Q Learning + DNN</li>
<li>Experience Replay</li>
<li>Target Network</li>
</ul>
</li>
<li>
<h3 id="算法流程">算法流程</h3>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/dqn-flow.png?raw=true" alt="dqn"></li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="double-dqn">Double DQN</h2>
<ul>
<li>
<h3 id="核心思路">核心思路</h3>
<ul>
<li>DQN中的TD目标值<!-- raw HTML omitted -->存在max操作，会引入一个正向偏差</li>
<li>因此建模两个Q网络，一个用于选动作，一个用于评估动作：
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>其实只要把DQN的target network也变成独立更新的就行了</li>
</ul>
</li>
<li>
<h3 id="算法流程-1">算法流程</h3>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/double-dqn-flow.png?raw=true" alt="double-dqn"></li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="dueling-network">Dueling Network</h2>
<ul>
<li>
<h3 id="核心思路-1">核心思路</h3>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/dueling-dqn.png?raw=true" alt="dueling-dqn"></li>
<li>用一个网络，分别学习V函数和A(优势)函数，最后相加得到Q函数</li>
</ul>
</li>
<li>
<h3 id="优势">优势</h3>
<ul>
<li>对于很多状态，不需要估计每个动作的Q值，每来一个样本都可以更新一次V函数，V函数的学习机会比Q函数高很多，- 泛化性能好，当新的动作进来时，不需要重新学习V，只需要重新学习A</li>
<li>减少了Q函数由于状态和动作维度差导致的噪声和突变</li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="prioritized-experience-replay">Prioritized Experience Replay</h2>
<ul>
<li>
<h3 id="核心思路-2">核心思路</h3>
<ul>
<li>DQN是从memory中均匀的采样，有时候，我们希望更多的去采样对学习有帮助的片段，给不同的experience提供不同的权重</li>
<li>利用TD误差去衡量权重</li>
<li>需要使用sum-tree以及binary heap data structure去实现</li>
<li>为了保证新加入的样本至少被采样一次，新样本的TD误差会被设置为最大</li>
<li>类似于DP中的优先清理</li>
<li>Experience Replay 使得更新不受限于实际经验的顺序</li>
<li>Prioritized Experience Replay 使得更新不受限于实际经验的频率</li>
</ul>
</li>
<li>
<h3 id="存在问题">存在问题</h3>
<ul>
<li>TD 误差对噪声敏感</li>
<li>TD 误差小的 transition 长时间不更新</li>
<li>过分关注 TD 误差大的 transition 丧失了样本多样性 使用某种分布采样了 Experience, 会引入 Bias</li>
</ul>
</li>
<li>
<h3 id="解决方法">解决方法</h3>
<ul>
<li>两种变体：
<ul>
<li>
<!-- raw HTML omitted -->
</li>
<li>其中，<!-- raw HTML omitted -->表示TD误差，<!-- raw HTML omitted -->表示人为施加的噪声，通过这个噪声保证多样性</li>
<li><!-- raw HTML omitted -->，序号的倒数来表示权重，对噪声的敏感度下降。简单的说，即使TD误差有0.1的误差，但顺序上依然是排第二，这时，误差就没有影响了。</li>
</ul>
</li>
<li>重要性采样，消除Bias</li>
</ul>
</li>
<li>
<h2 id="算法流程-2">算法流程</h2>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/prioritized-exp-replay.png?raw=true" alt="prioritized-exp-replay"></li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="rainbow">Rainbow</h2>
<ul>
<li>
<h3 id="核心思想">核心思想</h3>
<ul>
<li>将大量的前人的工作汇总实现，明确每一种改进所对应的效果，采用的工作有
<ul>
<li>DQN</li>
<li>Double DQN</li>
<li>Dueling DQN</li>
<li>Prioritized Experience Replay</li>
<li>NoiseNet(Noisy Netwoks for Exploration, AAAI2018)</li>
<li>Distributional RL(A Distributional Perspective on Reinforcement Learning, 2017)</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="效果">效果</h3>
<ul>
<li>Rainbow的效果比所有的base line好</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/rainbow.png?raw=true" alt="Rainbow"></li>
</ul>
</li>
</ul>
</li>
</ul>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/dynamic-planning/">Dynamic Planning</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="概念定义">概念定义</h1>
<ul>
<li>当有一个精确的环境模型时,可以用动态规划去解</li>
<li>将问题分解成子问题,通过解决子问题,来解决原问题</li>
<li>贝尔曼方程是关键</li>
</ul>
<h1 id="问题特性">问题特性</h1>
<ul>
<li>
<h2 id="最优子结构">最优子结构</h2>
<ul>
<li>满足最优性原理：不论初始状态和初始决策如何,对于前面决策所造成的某一状态而言,其后各阶段的决策序列必须构成最优策略</li>
<li>最优的解可以被分解成子问题的最优解</li>
</ul>
</li>
<li>
<h2 id="交叠式子问题">交叠式子问题</h2>
<ul>
<li>子问题能够被多次重复</li>
<li>子问题的解要能够被缓存并再利用</li>
</ul>
</li>
</ul>
<h1 id="策略评价">策略评价</h1>
<ul>
<li>
<h2 id="问题">问题</h2>
</li>
</ul>
<blockquote>
<p>给定一个策略<!-- raw HTML omitted -->,求对应的值函数<!-- raw HTML omitted -->或者<!-- raw HTML omitted --></p></blockquote>
<ul>
<li>
<h2 id="方法">方法</h2>
<ul>
<li>直接解
<ul>
<li>可以直接求得精确解</li>
<li>时间复杂度</li>
</ul>
</li>
<li>迭代解
<ul>
<li>利用贝尔曼期望方程迭代求解</li>
<li>可以收敛到精确解</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="策略提升">策略提升</h1>
<ul>
<li>根据现有的策略评价结果<!-- raw HTML omitted -->改进策略<!-- raw HTML omitted --></li>
<li>典型的如贪婪策略提升</li>
</ul>
<h1 id="策略迭代">策略迭代</h1>
<ul>
<li>
<h2 id="流程">流程</h2>
<ul>
<li>{策略评价(迭代k次，直到接近收敛为止) + 策略提升(提升1次)}; &hellip;</li>
<li>v1 → π1 → v2 → π2 → v3 → π3 → &hellip;</li>
</ul>
</li>
<li>
<h2 id="终止条件">终止条件</h2>
<ul>
<li>提升停止</li>
</ul>
</li>
<li>
<h2 id="特点">特点</h2>
<ul>
<li>有显式的策略</li>
<li>迭代过程中的值函数对应了某个具体的策略</li>
<li>效率较低</li>
<li>贝尔曼期望方程 + 贪婪策略提升</li>
</ul>
</li>
</ul>
<h1 id="值迭代">值迭代</h1>
<ul>
<li>
<h2 id="流程-1">流程</h2>
<ul>
<li>策略评价(迭代1次); &hellip;</li>
<li>v1 → v2 → v3 → &hellip;</li>
</ul>
</li>
<li>
<h2 id="特点-1">特点</h2>
<ul>
<li>没有显式的策略</li>
<li>迭代过程中的值函数可能不对应任何策略</li>
<li>效率较高</li>
<li>贝尔曼期望方程</li>
</ul>
</li>
</ul>
<h1 id="扩展">扩展</h1>
<ul>
<li>
<h2 id="异步动态规划">异步动态规划</h2>
<ul>
<li>以某种顺序单独考虑每一个状态</li>
<li>能够大大减少计算量</li>
<li>只要所有的状态都能被持续的选择到,收敛性能够保证</li>
<li>
<h3 id="常用的三种形式">常用的三种形式</h3>
<ul>
<li>就地(In-Place)动态规划</li>
<li>优先清理</li>
<li>实时动态规划</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="就地in-place动态规划">就地(In-Place)动态规划</h1>
<ul>
<li>同步值迭代存储了两个版本的值函数，在计算<!-- raw HTML omitted -->的时候，使用了<!-- raw HTML omitted -->的复制版本，在整个更新过程中，<!-- raw HTML omitted -->是不变的，保持上一个循环的状态。</li>
<li>就地(In-Place)动态规划只对一个值函数<!-- raw HTML omitted -->进行更新，因此，从左上角开始更新和从右下角开始更新，得到的结果是不一样的。</li>
</ul>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/markov-decision-processes/">Markov Decision Processes</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="mdps">MDPs</h1>
<ul>
<li>
<h2 id="马尔可夫性">马尔可夫性</h2>
<ul>
<li>只要知道现在，将来和过去条件独立</li>
<li>每一时刻的状态只与上一时刻的状态有关</li>
<li>当前状态包含了所有的历史状态信息</li>
<li>要求环境全观测</li>
</ul>
</li>
<li>
<h2 id="任务类型定义">任务类型定义</h2>
<ul>
<li>强化学习中,从初始状态<!-- raw HTML omitted -->到终止状态的序列过程,被称为一个片段(episode)。
<ul>
<li>如果一个任务总以终止状态结束,那么这个任务被称为片段任务(episodic task)</li>
<li>如果一个任务会没有终止状态,会被无限执行下去,这被称为连续性任务 (continuing task)</li>
</ul>
</li>
<li>终止状态等价于自身转移概率为 1,奖励为 0 的的状态</li>
</ul>
</li>
<li>
<h2 id="状态转移矩阵">状态转移矩阵</h2>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>s1</th>
          <th>s2</th>
          <th>s3</th>
          <th>s4</th>
          <th>转移</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0.5</td>
          <td>0.0</td>
          <td>0.5</td>
          <td>0.0</td>
          <td>s1</td>
      </tr>
      <tr>
          <td>0.1</td>
          <td>0.2</td>
          <td>0.3</td>
          <td>0.4</td>
          <td>s2</td>
      </tr>
      <tr>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>s3</td>
      </tr>
      <tr>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>s4</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>上图中s1转换到s1的概率是0.5，转换到s3的概率是0.5；s2转换到s1的概率是0.1，转换到s2的概率是0.2，转换到s3的概率是0.3，转换到s4的概率是0.4。</p></blockquote>
<ul>
<li>
<h2 id="奖励与回报">奖励与回报</h2>
<ul>
<li>
<h3 id="奖励值对每一个状态的评价">奖励值:对每一个状态的评价</h3>
</li>
<li>
<h3 id="回报值-对每一个片段的评价">回报值: 对每一个片段的评价</h3>
<ul>
<li>对于片断性任务，回报值是未来有限个状态的奖励值的和<!-- raw HTML omitted --></li>
<li>对于连续性任务，回报值是未来无限个状态的奖励值的和<!-- raw HTML omitted --></li>
<li>回报值是从时间<!-- raw HTML omitted -->处开始的累计衰减奖励</li>
</ul>
</li>
<li>
<h3 id="指数衰减值">指数衰减值</h3>
<ul>
<li>对未来的把握也是逐渐衰减的</li>
<li>一般情况下,我们更关注短时间的反馈</li>
</ul>
</li>
<li>
<h3 id="值函数某个状态所对应回报值的期望">值函数:某个状态所对应回报值的期望</h3>
</li>
</ul>
</li>
<li>
<h2 id="贝尔曼方程">贝尔曼方程</h2>
<ul>
<li>强化学习的核心</li>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h2 id="策略">策略</h2>
<ul>
<li>状态值函数(<!-- raw HTML omitted -->)：是从状态 s 开始，使用策略 π 得到的期望回报值</li>
<li>状态动作值函数(<!-- raw HTML omitted -->)：是从状态 s 开始,执行动作 a，然后使用策略 π 得到的期望回报值</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h2 id="知识点">知识点</h2>
<ul>
<li>贝尔曼最优方程不是线性的</li>
<li>一般很难有闭式的解</li>
<li>可以使用迭代优化的方法去解
<ul>
<li>值迭代</li>
<li>策略迭代</li>
<li>Q 学习</li>
<li>SARSA</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="pomdp">POMDP</h1>
<ul>
<li>观测不等于状态O ≠ S</li>
<li>POMDPs 由七元组构成 &lt; S, A, O, P, R, Z, γ &gt;</li>
<li>Z是观测函数</li>
<li>观测不满足马尔可夫性,因此也不满足贝尔曼方程</li>
<li>状态未知,隐马尔可夫过程</li>
<li>有时对于 POMDPs 来说,最优的策略是随机性的</li>
</ul>
<h1 id="无衰减-mdps">无衰减 MDPs</h1>
<ul>
<li>用于各态历经马尔可夫决策过程</li>
<li>存在独立于状态的平均奖赏</li>
<li>求值函数时,需要减去该平均奖赏,否则有可能奖赏爆炸</li>
</ul>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/model-based-rl/">Model based RL</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="前言">前言</h1>
<ul>
<li>强化学习可以分为<strong>有模型</strong>和<strong>无模型</strong>的方法两大类</li>
<li>未知模型
<ul>
<li>学习法</li>
<li>通过智能体的交互，学习值函数和策略</li>
<li>代表方法：MC，TD</li>
</ul>
</li>
<li>已知模型
<ul>
<li>规划法</li>
<li>无需智能体交互，直接从模型学习最优策略</li>
<li>代表方法：DP</li>
</ul>
</li>
</ul>
<h1 id="基于模型的强化学习">基于模型的强化学习</h1>
<h2 id="核心思路">核心思路</h2>
<ul>
<li>通过经验，学习出一个虚拟的环境模型</li>
<li>利用学到的环境模型，进行动态规划，计算价值函数或者策略</li>
</ul>
<h2 id="优势">优势</h2>
<ul>
<li>可以通过监督学习，有效地学习环境模型</li>
<li>可以将学到的环境模型放在GPU内，快速得到大量的交互信息</li>
<li>没有任何真实损失</li>
<li>直接利用环境模型的不确定性</li>
</ul>
<h2 id="劣势">劣势</h2>
<ul>
<li>先学环境模型，再学值函数，存在两次近似误差 -&gt; 累计误差</li>
</ul>
<h2 id="dyna-q">Dyna-q</h2>
<ul>
<li>算法流程
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/dyna-q-flow.png?raw=true" alt="流程图"></li>
<li>repeat n times的部分实际上是在用学习到的虚拟环境模型进行学习，这一部分可以新开一个线程，和外面真实的环境交互分开。</li>
<li>真实的环境交互1秒一次，而计算机中虚拟的环境交互0.1秒一次，那这里的n就是10。</li>
</ul>
</li>
</ul>
<h2 id="蒙特卡洛树搜索monte-carlo-tree-search">蒙特卡洛树搜索Monte-Carlo Tree Search</h2>
<ul>
<li>
<h3 id="特性">特性</h3>
<ul>
<li>适用于Combinatorial Games</li>
<li>Combinatorial Games特点：零和、完美信息、确定性、离散、序列化</li>
</ul>
</li>
<li>
<h3 id="算法核心">算法核心</h3>
<ul>
<li>在MCTS中，仿真策略需要策略提升</li>
<li>每次仿真有两个阶段
<ul>
<li>树策略(提升)：选择动作，最大化Q(S, A)</li>
<li>默认策略(固定)：快速计算到终止状态</li>
</ul>
</li>
<li>Repeat(每次仿真)
<ul>
<li>使用MC评价来估计Q(S, A)</li>
<li>提升树策略，比如epsilon-贪婪，UCB等</li>
</ul>
</li>
<li>对仿真出来的经验做MC优化</li>
<li>搜索到最优的搜索树Q(S,A)-&gt;q*(S,A)</li>
</ul>
<blockquote>
<p>epsilon-贪婪对于非最优策略是均匀采样的
UCB既考虑了值函数，又考虑了探索的次数 = 回报值/仿真次数</p></blockquote>
</li>
<li>
<h3 id="优势-1">优势</h3>
<ul>
<li>Highly selective best-first search</li>
<li>动态评价状态</li>
<li>结合了采样去打破维度诅咒，用采样取代了暴力搜索</li>
<li>适合于各种黑盒模型，不需要满足马尔可夫性</li>
<li>计算有效，容易并行</li>
</ul>
</li>
<li>
<h3 id="算法流程">算法流程</h3>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-1.png?raw=true" alt="MCTS1">
<ul>
<li>当前状态S1为叶子节点，直接从当前状态开始，用默认策略进行仿真</li>
<li>结果“赢了”</li>
<li>更新节点的UCB=获胜次数/仿真次数</li>
</ul>
</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-2.png?raw=true" alt="MCTS2">
<ul>
<li>依据UCB规则选择一个动作向下走，每一个动作都对应了一个UCB</li>
<li>UCB其实是一个值，与值函数正相关，与仿真次数负相关</li>
<li>但是目前仿真次数都是0，所以根据值函数选一个动作，到达S2</li>
<li>结果“输了”</li>
<li>更新树策略路径上的每个节点的UCB</li>
</ul>
</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-3.png?raw=true" alt="MCTS3">
<ul>
<li>从S1开始，根据UCB选一个动作</li>
<li>由于第二步中，最终结果是“输了”，所以第二步中动作的UCB下降了，于是从S1重新选一个动作，到达S3</li>
<li>结果“赢了”</li>
<li>更新树策略路径上的每个节点的UCB</li>
</ul>
</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-4.png?raw=true" alt="MCTS4">
<ul>
<li>再从S1开始搜索，搜到S3后，再选一个动作，到达S4</li>
<li>结果“输了”</li>
<li>更新树策略路径上的每个节点的UCB</li>
</ul>
</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-5.png?raw=true" alt="MCTS5">
<ul>
<li>再从S1开始搜索，搜到S3后，没有去S4(因为S4刚刚输了，UCB下降)，再选一个动作，到达S5</li>
<li>结果“赢了”</li>
<li>更新树策略路径上的每个节点的UCB</li>
</ul>
</li>
<li>
<h4 id="后面的步骤依次类推">后面的步骤依次类推</h4>
</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-flow.png?raw=true" alt="MCTS-FLOW"></li>
</ul>
</li>
</ul>
<h2 id="td搜索">TD搜索</h2>
<ul>
<li>
<h3 id="算法特性">算法特性</h3>
</li>
<li>有些情况下，没有终止状态，MC方法不适用</li>
<li>将MCTS中的MC评价换成TD评价</li>
<li>
<h3 id="算法流程-1">算法流程</h3>
<ul>
<li>从当前状态St开始采样片段</li>
<li>估计Q(s, a)</li>
<li>对于每一步的仿真，使用Sarsa方法更新Q函数
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>基于Q(s, a)选择动作</li>
</ul>
</li>
</ul>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/monte-carlo-sampling/">Monte-Carlo Sampling</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="基本概念">基本概念</h1>
<ul>
<li>蒙特卡洛采样是无模型方法</li>
<li>行为策略是智能体与环境交互的策略</li>
<li>目标策略是我们要学习的策略</li>
<li>
<h2 id="在策略on-policy学习">在策略（on-policy）学习</h2>
<ul>
<li>行为策略和目标策略是同一个策略</li>
<li>直接使用样本统计属性去估计总体</li>
<li>更简单,且收敛性更好</li>
<li>数据利用性更差(只有智能体当前交互的样本能够被利用)</li>
<li>限定了学习过程中的策略是随机性策略</li>
</ul>
</li>
<li>
<h2 id="离策略off-policy学习">离策略（off-policy）学习</h2>
<ul>
<li>行为策略和目标策略不是同一个策略</li>
<li>一般行为策略选用随机性策略，目标策略选用确定性策略</li>
<li>需要结合重要性采样才能使用样本估计总体</li>
<li>方差更大,收敛性更差</li>
<li>数据利用性更好 (可以使用其他智能体交互的样本)</li>
<li>行为策略需要比目标策略更具备探索性</li>
</ul>
</li>
<li>
<h2 id="重要性采样">重要性采样</h2>
<ul>
<li>是一种估计概率分布期望值的技术,它使用了来自其他概率分布的样本</li>
<li>主要用于无法直接采样原分布的情况</li>
<li>估计期望值时,需要加权概率分布的比值</li>
</ul>
</li>
</ul>
<h1 id="算法特性">算法特性</h1>
<ul>
<li>MC方法可以被用于任意涉及随机变量的估计</li>
<li>这里MC方法特指利用统计平均估计期望值的方法</li>
<li>MC方法从完整的片段中学习</li>
<li>MC方法仅仅用于片段性任务(必须有终止条件)</li>
</ul>
<h1 id="算法核心">算法核心</h1>
<blockquote>
<p>通过不断的采样,然后统计平均回报值来估计值函数,方差较大</p></blockquote>
<ul>
<li>从某个状态S开始，通过某种策略P进行探索，一直到终止状态，得到反馈Fk</li>
<li>重复以上步骤n次，V(s)=(F1+F2+&hellip;+Fn)/n</li>
</ul>
<h1 id="蒙特卡洛评价">蒙特卡洛评价</h1>
<ul>
<li>首次拜访(First-visit)MC策略评价</li>
<li>每次拜访(Every-visit)MC策略评价</li>
</ul>
<pre tabindex="0"><code>s1,s2,s3,s1,s4,s2,s5 +1
s1,s2,s1,s5 +1
对于上面的两种采样轨迹，评价s1时，首次拜访只在s1在一条轨迹中第一次出现时N=N+1；每次拜访则是出现一次s1就N=N+1
首次拜访：(1+1)/2 = 1
每次拜访：(1+1)/4 = 0.25
</code></pre><h1 id="q函数的mc方法">Q函数的MC方法</h1>
<ul>
<li>每次是针对一个s和一个a进行评价</li>
<li>为了充分探索所有的s,a组合，随机选择初始状态和初始动作</li>
</ul>
<h1 id="离策略的mc策略评价">离策略的MC策略评价</h1>
<ul>
<li>核心是利用重要性采样去加权回报值</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>使用重要性采样会显著增加方差, 可能到无限大</li>
</ul>
<h1 id="mc小结">MC小结</h1>
<ul>
<li>偏差为 0,是无偏估计</li>
<li>方差较大,需要大量数据去消除</li>
<li>收敛性较好</li>
<li>没有利用马尔可夫性,有时可以用在非马尔可夫环境</li>
</ul>
<hr>
<h1 id="增量式mc">增量式MC</h1>
<blockquote>
<p>之前的蒙特卡洛算法需要采样大量轨迹之后再统一计算平均数，能不能在每一条轨迹之后都得到值函数的估计值呢?</p></blockquote>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>这里的<!-- raw HTML omitted -->可以认为是更新的步长</li>
<li>很多时候，我们会把<!-- raw HTML omitted -->替换为一个常数<!-- raw HTML omitted -->，好处如下：
<ul>
<li>会逐渐遗忘过去的轨迹</li>
<li>对初始值敏感度更小</li>
<li>适用于不稳定环境</li>
</ul>
</li>
</ul>
<h1 id="mc策略提升">MC策略提升</h1>
<ul>
<li>不能使用贪婪策略提升，会导致部分状态永远不会遍历到</li>
<li>每次探索，有一定的几率随机选择动作，其他情况下都采取贪婪策略</li>
</ul>
<h1 id="无限探索下的极限贪婪glie">无限探索下的极限贪婪(GLIE)</h1>
<ul>
<li>无限探索:所有的状态动作对能够被探索无穷次</li>
<li>极限贪婪:在极限的情况下,策略会收敛到一个贪婪的策略</li>
</ul>
<blockquote>
<p>GLIE 蒙特卡洛优化能收敛到最优的 Q 函数</p>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/policy-gradient/">Policy Gradient</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="强化学习分类">强化学习分类</h1>
<ul>
<li>
<h2 id="策略梯度算法">策略梯度算法</h2>
<ul>
<li>直接用神经网络表示策略</li>
<li>神经网络输出N维的向量，每一维表示选择该动作的概率大小</li>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h2 id="值函数算法">值函数算法</h2>
<ul>
<li>用神经网络拟合Q或者V函数</li>
<li>得到Q之后，利用贪婪策略等选择下一步动作</li>
<li><!-- raw HTML omitted -->或者<!-- raw HTML omitted --></li>
</ul>
</li>
<li>
<h2 id="actor-critic">Actor-Critic</h2>
<ul>
<li>学习值函数</li>
<li>学习策略</li>
<li>介于上面两者之间</li>
</ul>
</li>
</ul>
<h1 id="策略梯度算法优缺点">策略梯度算法优缺点</h1>
<ul>
<li>
<h2 id="优点">优点</h2>
<ul>
<li>更好的收敛性</li>
<li>有效处理高维和连续的动作空间</li>
<li>能够学到随机策略</li>
<li>不会导致策略退化</li>
</ul>
</li>
<li>
<h2 id="缺点">缺点</h2>
<ul>
<li>容易陷入局部最优</li>
<li>难以评价一个策略，评价结果方差很大</li>
</ul>
</li>
</ul>
<h1 id="策略退化">策略退化</h1>
<ul>
<li>模型的能力不够导致</li>
<li>值函数估计不准导致</li>
</ul>
<hr>
<h3 id="我们从经典的a2c算法入手讲解策略梯度算法">我们从经典的A2C算法入手讲解策略梯度算法</h3>
<h1 id="优化目标">优化目标</h1>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
<li>其中，<!-- raw HTML omitted -->表示策略网络的参数；<!-- raw HTML omitted -->表示一段状态转移轨迹；<!-- raw HTML omitted -->表示该轨迹的最终回报值；<!-- raw HTML omitted -->表示当策略网络的参数为<!-- raw HTML omitted -->时，出现<!-- raw HTML omitted -->的概率大小。</li>
<li>在一个固定的环境再，一般来说，<!-- raw HTML omitted -->是稳定不变的。</li>
</ul>
<h1 id="优化方法">优化方法</h1>
<ul>
<li>
<h2 id="梯度表达式">梯度表达式</h2>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h2 id="似然率角度梯度求解">似然率角度梯度求解</h2>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/likehood-solver.png?raw=true" alt="LIKEHOOD"></li>
</ul>
</li>
<li>
<h2 id="似然率梯度的理解">似然率梯度的理解</h2>
<ul>
<li><!-- raw HTML omitted -->是轨迹<!-- raw HTML omitted -->的出现概率随<!-- raw HTML omitted -->变化最陡的方向。
<ul>
<li>沿正方向，轨迹出现的概率会变大</li>
<li>沿负方向，轨迹出现的概率会变小</li>
</ul>
</li>
<li><!-- raw HTML omitted -->控制了参数更新的方向和步长，R是正的，就让轨迹出现的概率变大，并且R越大，步长的幅度越大；相反亦然。</li>
<li>最终增大了高回报率轨迹出现的概率，减少了低回报率轨迹出现的概率</li>
</ul>
</li>
<li>
<h2 id="轨迹分解到状态">轨迹分解到状态</h2>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/decompose.png?raw=true" alt="decompose"></li>
</ul>
</li>
<li>
<h2 id="算法流程">算法流程</h2>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/reinforce.png?raw=true" alt="reinforce"></li>
</ul>
</li>
</ul>
<h1 id="actor-critic-1">Actor-Critic</h1>
<ul>
<li>上面的reinforce算法中，<!-- raw HTML omitted -->的方差非常大，为了减小方差，我们引入了Critic函数<!-- raw HTML omitted -->代替<!-- raw HTML omitted --></li>
<li>再进一步，由于每个Q都是正的，会导致网络对于任何轨迹都想提高其出现的概率，因此，引入一个基线。基线的选择为当前状态的V值。由此得到一个优势函数：</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>上面的方法需要设计一个Q函数一个V函数，为了简化，我们直接用TD误差代替优势函数。TD误差为：</li>
<li><!-- raw HTML omitted -->其中，<!-- raw HTML omitted -->是<!-- raw HTML omitted -->的后一个状态</li>
</ul>
<h1 id="总结">总结</h1>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/conclusion.png?raw=true" alt="reinforce"></li>
<li>其中，Advantage Actor-Critic又叫A2C，由于TD Actor-Critic是Advantage Actor-Critic的无偏估计，所以实际在使用A2C的时候，都是用的TD Actor-Critic</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/a2c.png?raw=true" alt="A2C"></li>
<li>A2C需要多进程来打破训练数据之间的相关性</li>
</ul>
<h1 id="其他策略梯度算法简单介绍">其他策略梯度算法简单介绍</h1>
<ul>
<li>
<h2 id="确定性梯度策略算法dpg">确定性梯度策略算法DPG</h2>
<ul>
<li>
<h3 id="特性">特性</h3>
<ul>
<li>直接采用确定性动作输出：<!-- raw HTML omitted --></li>
<li>可以用于高维和连续动作的情况</li>
<li>常规的策略梯度方法无法用到高维和连续动作空间</li>
</ul>
</li>
<li>
<h3 id="梯度求解">梯度求解</h3>
<ul>
<li>过去一直认为无模型情况下确定性策略梯度不存在</li>
<li>DPG证明了梯度存在，并建立了其与Q函数的关系</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/dpg-gradient.png?raw=true" alt="dpg-gradient"></li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="ddpg">DDPG</h2>
<ul>
<li>
<h3 id="核心思路">核心思路</h3>
<ul>
<li>Continuous Control with Deep Reinforcement Learning (ICRL2016)</li>
<li>结合了 DQN 和 DPG</li>
<li>利用随机过程产生探索性动作</li>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/ddpg.png?raw=true" alt="ddpg"></li>
</ul>
</li>
<li>
<h3 id="算法流程-1">算法流程</h3>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/ddpg-flow.png?raw=true" alt="ddpg-flow"></li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="a3c">A3C</h2>
<ul>
<li>
<h3 id="论文来源">论文来源</h3>
<ul>
<li>Asynchronous Methods for Deep Reinforcement Learning (ICML2016)</li>
</ul>
</li>
<li>
<h3 id="问题提出">问题提出</h3>
<ul>
<li>Online 的算法和 DNN 结合后不稳定 (样本关联性)</li>
</ul>
</li>
<li>
<h3 id="解决方案">解决方案</h3>
<ul>
<li>创建多个agent，在多个环境中执行异步学习构建batch(多线程)
<ul>
<li>来自不同环境的样本无相关性</li>
<li>不依赖于 GPU 和大型分布式系统</li>
<li>不同线程使用了不同的探索策略，增加了探索量</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="算法流程-2">算法流程</h3>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/a3c.png?raw=true" alt="a3c"></li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="a2c">A2C</h2>
<ul>
<li>
<h3 id="来源">来源</h3>
<ul>
<li>OpenAI对A3C进行了改进，把<strong>异步</strong>变成了<strong>同步</strong>，等所有线程的动作执行完毕得到reward后一起拿来更新，可以用GPU完成该动作，效率高</li>
<li>当batch_size较大时效果好</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="策略梯度知识图谱">策略梯度知识图谱</h1>
<ul>
<li><img src="https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/pg-graph.png?raw=true" alt="pg-graph"></li>
</ul>
<h1 id="扩展其他策略梯度算法">扩展(其他策略梯度算法)</h1>
<ul>
<li>自然梯度法：寻找策略更新最快的方向</li>
<li>信赖域策略优化算法(TRPO)：研究了更新步长的选择，步长选择在策略梯度中非常重要，但实现非常复杂</li>
<li>近端策略优化(PPO)：对TRPO的改进，使实现非常简单，实际使用中，效果比较好甚至最好的方案</li>
</ul>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/abstract/">Reinforcement Learning</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="强化学习基础"><a href="https://xwlu.github.io/wiki/reinforcement-learning/common/">强化学习基础</a></h1>
<h1 id="马尔科夫过程"><a href="https://xwlu.github.io/wiki/reinforcement-learning/markov-decision-processes/">马尔科夫过程</a></h1>
<h1 id="动态规划"><a href="https://xwlu.github.io/wiki/reinforcement-learning/dynamic-planning/">动态规划</a></h1>
<h1 id="无模型方法">无模型方法</h1>
<blockquote>
<p>无模型方法：未知环境模型，需要通过和环境的交互获得反馈进行策略学习。有模型方法：已知环境模型，根据环境直接推导最优策略。</p></blockquote>
<ul>
<li>
<h2 id="蒙特卡洛"><a href="https://xwlu.github.io/wiki/reinforcement-learning/monte-carlo-sampling/">蒙特卡洛</a></h2>
</li>
<li>
<h2 id="时间差分"><a href="https://xwlu.github.io/wiki/reinforcement-learning/temporal-difference/">时间差分</a></h2>
</li>
<li>
<h2 id="多步自举"><a href="https://xwlu.github.io/wiki/reinforcement-learning/temporal-difference-lambda/">多步自举</a></h2>
</li>
<li>
<h2 id="基于策略梯度的强化学习"><a href="https://xwlu.github.io/wiki/reinforcement-learning/policy-gradient">基于策略梯度的强化学习</a></h2>
</li>
<li>
<h2 id="dqn"><a href="https://xwlu.github.io/wiki/reinforcement-learning/dqn">DQN</a></h2>
</li>
<li>
<h2 id="基于信赖域的强化学习"><a href="https://xwlu.github.io/wiki/reinforcement-learning/trust-region-based-drl">基于信赖域的强化学习</a></h2>
</li>
</ul>
<h1 id="基于模型的强化学习"><a href="https://xwlu.github.io/wiki/reinforcement-learning/model-based-rl/">基于模型的强化学习</a></h1>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/temporal-difference/">Temporal Difference</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="temporal-difference">Temporal Difference</h1>
<ul>
<li>增量式MC
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>时间差分TD
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>核心差别：
<ul>
<li>MC是根据[s1→终止状态]完整片段的最终回报更新s1的值函数</li>
<li>TD是根据[s1→s2]这一步片段的即时回报值R和s2的估计值函数更新s1的值函数</li>
</ul>
</li>
</ul>
<h1 id="与dp的对比">与DP的对比</h1>
<ul>
<li>DP是全宽备份</li>
<li>TD是样本备份</li>
</ul>
<h1 id="td与mc的对比">TD与MC的对比</h1>
<ul>
<li><strong>TD 算法在知道结果之前学习</strong>
<ul>
<li>TD算法在每一步之后都能在线学习</li>
<li>MC算法必须等待最终回报值得到之后才能学习</li>
</ul>
</li>
<li><strong>TD算法即便没有最终结果也能学习</strong>
<ul>
<li>TD算法能够从不完整序列中学习</li>
<li>MC算法仅仅能够从完整序列中学习</li>
<li>TD算法适用于连续性任务和片段性任务</li>
<li>MC算法仅仅适用于片段性任务</li>
</ul>
</li>
<li><strong>TD算法有多个驱动力</strong>
<ul>
<li>MC算法只有奖励值作为更新的驱动力</li>
<li>TD算法有奖励值和状态转移作为更新的驱动力</li>
</ul>
</li>
<li><strong>MC有高方差,零偏差</strong>
<ul>
<li>收敛性较好 (即使采用函数逼近)</li>
<li>对初始值不太敏感</li>
<li>随着样本数量的增加,方差逐渐减少, 趋近于 0</li>
</ul>
</li>
<li><strong>TD 有低方差,和一些偏差</strong>
<ul>
<li>通常比 MC 效率更高</li>
<li>表格法下TD(0)收敛到<!-- raw HTML omitted -->(函数逼近时不一定)</li>
<li>对初始值更敏感</li>
<li>随着样本数量的增加,偏差逐渐减少,趋近于 0</li>
<li>样本数量有限时，TD的结果与真实结果的偏差比较稳定。MC可能出现巨大偏差。</li>
</ul>
</li>
<li><strong>TD要求环境符合马尔科夫性，MC不要求</strong></li>
</ul>
<h1 id="自举和采样">自举和采样</h1>
<ul>
<li><strong>自举</strong>: 使用随机变量的估计去更新
<ul>
<li>MC 没有自举</li>
<li>DP 和 TD 都有自举</li>
</ul>
</li>
<li><strong>采样</strong>: 通过样本估计期望
<ul>
<li>MC 和 TD 采样</li>
<li>DP 不采样</li>
</ul>
</li>
</ul>
<h1 id="td的优化方法">TD的优化方法</h1>
<ul>
<li>整体思路是 策略评价+策略提升</li>
<li>
<h2 id="策略评价">策略评价</h2>
<ul>
<li>
<h3 id="在策略评价sarsa">在策略评价<strong>SARSA</strong></h3>
<ul>
<li>
<h4 id="公式">公式</h4>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h4 id="算法流程">算法流程</h4>
<ul>
<li>1:初始化<!-- raw HTML omitted -->且<!-- raw HTML omitted --></li>
<li>2:repeat(对于每个片段)</li>
<li>3:  初始化状态<!-- raw HTML omitted --></li>
<li>4:  根据<!-- raw HTML omitted -->选择一个在<!-- raw HTML omitted -->处的动作<!-- raw HTML omitted -->(使用<!-- raw HTML omitted -->-贪婪策略)</li>
<li>5:  repeat(对于片段中每一步)</li>
<li>6:    执行动作<!-- raw HTML omitted -->，观测<!-- raw HTML omitted --></li>
<li>7:    根据<!-- raw HTML omitted -->选择一个在<!-- raw HTML omitted -->处的动作<!-- raw HTML omitted -->(使用<!-- raw HTML omitted -->-贪婪策略)</li>
<li>8:    <!-- raw HTML omitted --></li>
<li>9:    <!-- raw HTML omitted --></li>
<li>10:  until <!-- raw HTML omitted -->是终止状态</li>
<li>11:until收敛</li>
</ul>
</li>
<li>
<h4 id="收敛性">收敛性</h4>
<ul>
<li>在满足以下条件时,Sarsa 算法收敛到最优的状态动作值函数
<ul>
<li>策略序列<!-- raw HTML omitted -->满足GLIE</li>
<li>步长序列<!-- raw HTML omitted -->是一个Robbins-Monro序列
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
</ul>
</li>
<li>GLIE 保证了
<ul>
<li>充分的探索</li>
<li>策略最终收敛到贪婪的策略</li>
</ul>
</li>
<li>Robbins-Monro保证了
<ul>
<li>步长足够大,足以克服任意初始值</li>
<li>步长足够小,最终收敛 (常量步长不满足)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="期望sarsa">期望SARSA</h3>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
<li>减少了由于<!-- raw HTML omitted -->的选择带来的方差</li>
<li>在相同更新步数时,期望 Sarsa 比 Sarsa 的通用性更好</li>
<li>可以在在策略和离策略中切换
<ul>
<li>在策略:TD目标值中的<!-- raw HTML omitted -->中的策略<!-- raw HTML omitted -->和采样的策略是同一个策略</li>
<li>离策略:TD目标值中的<!-- raw HTML omitted -->中的策略<!-- raw HTML omitted -->和采样的策略是不同的策略</li>
</ul>
</li>
<li>一种特殊情况,TD目标值中的策略选择贪婪策略, 采样的策略选用ε-贪婪策略——<strong>Q学习</strong></li>
</ul>
</li>
<li>
<h3 id="离策略评价q学习">离策略评价<strong>Q学习</strong></h3>
<ul>
<li>
<h4 id="公式-1">公式</h4>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<h4 id="算法流程-1">算法流程</h4>
<ul>
<li>1:初始化<!-- raw HTML omitted -->且<!-- raw HTML omitted --></li>
<li>2:repeat(对于每个片段)</li>
<li>3:  初始化状态<!-- raw HTML omitted --></li>
<li>4:  repeat(对于片段中每一步)</li>
<li>5:    根据<!-- raw HTML omitted -->选择一个在<!-- raw HTML omitted -->处的动作<!-- raw HTML omitted -->(使用<!-- raw HTML omitted -->-贪婪策略)</li>
<li>6:    执行动作<!-- raw HTML omitted -->，观测<!-- raw HTML omitted --></li>
<li>7:    <!-- raw HTML omitted --></li>
<li>8:    <!-- raw HTML omitted --></li>
<li>9:  until <!-- raw HTML omitted -->是终止状态</li>
<li>10:until收敛</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="策略提升">策略提升</h2>
<ul>
<li>
<h2 id="-贪婪策略提升"><!-- raw HTML omitted -->-贪婪策略提升</h2>
</li>
</ul>
</li>
</ul>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
        <li class="col-12 d-flex width-full py-4 border-bottom color-border-secondary public source">
          <div class="col-12 d-inline-block">
            <div class="d-inline-block mb-1">
              <h3 class="wb-break-all">
                <a href="http://localhost:1313/post/reinforcement-learning/temporal-difference-lambda/">Temporal Difference Lambda</a>
              </h3>
            </div>

            <div>
              <div class="col-12 d-inline-block text-gray mb-2 pr-4">
                <h1 id="temporal-difference-lambda">Temporal Difference Lambda</h1>
<ul>
<li>时间差分就是TD(0)算法，只向后采样一步；MC是向后采样整个片段；多步自举介于两者之间
<ul>
<li>TD: <!-- raw HTML omitted --></li>
<li>
<!-- raw HTML omitted -->
</li>
<li>MC: <!-- raw HTML omitted --></li>
</ul>
</li>
<li>经验认为<!-- raw HTML omitted -->步左右，要好于TD(0)和MC</li>
</ul>
<h1 id="n步td策略评价">n步TD策略评价</h1>
<ul>
<li>
<h2 id="算法流程">算法流程</h2>
<ul>
<li>1:repeat(对于每一个片段)</li>
<li>2:  repeat对于片段中的每一步</li>
<li>3:    根据<!-- raw HTML omitted -->选择动作<!-- raw HTML omitted --></li>
<li>4:    执行动作<!-- raw HTML omitted -->，观察到<!-- raw HTML omitted -->，并将其存储起来</li>
<li>5:    if <!-- raw HTML omitted -->, then</li>
<li>6:      <!-- raw HTML omitted --></li>
<li>7:      if <!-- raw HTML omitted -->,then <!-- raw HTML omitted --></li>
<li>8:      <!-- raw HTML omitted --></li>
<li>9:    end if</li>
<li>10:  until直到终止状态</li>
<li>11:until收敛</li>
</ul>
</li>
<li>
<h2 id="两个注意点">两个注意点</h2>
<ul>
<li>为了计算 n 步回报值,需要维护 R, S 的存储空间</li>
<li>对于后继状态不足 n 个的,使用 MC 目标值</li>
</ul>
</li>
</ul>
<h1 id="多步自举">多步自举</h1>
<h2 id="前向视角">前向视角</h2>
<ul>
<li>就是将TD(0),TD(1),&hellip;,TD(n)求平均</li>
<li>
<!-- raw HTML omitted -->
</li>
<li><!-- raw HTML omitted -->，退化成TD(0)；<!-- raw HTML omitted -->，退化成MC</li>
<li><!-- raw HTML omitted -->更新公式
<ul>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
</ul>
<h2 id="后向视角基于资格迹eligibility-traces">后向视角&ndash;基于资格迹(Eligibility Traces)</h2>
<ul>
<li>
<h3 id="基本概念">基本概念</h3>
<ul>
<li>状态转移片段：s1→s1→s1→s2→s3</li>
<li>信度分配(Credit assignment)问题:到底是s1还是s2造成了最后的s3
<ul>
<li>频率启发式: 归因到频数最高的状态</li>
<li>近因启发式: 归因到最近的状态</li>
<li>资格迹是两者的结合</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="资格迹的计算公式">资格迹的计算公式</h3>
<ul>
<li>
<!-- raw HTML omitted -->
</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>直观的感觉就是，第一次遇到这个状态s的时候，对它的记忆由0蹦到1，然后慢慢开始遗忘(对应<!-- raw HTML omitted -->这个动作)，下一次又遇到了，记忆就一下子清晰了(对应+1这个动作)，然后又慢慢遗忘。</li>
</ul>
</li>
<li>
<h3 id="利用资格迹实现多步自举">利用资格迹实现多步自举</h3>
<ul>
<li>对于每一个状态s，维护一个资格迹<!-- raw HTML omitted --></li>
<li>更新值函数V(s)时，会更新每一个状态s</li>
<li>使用TD误差<!-- raw HTML omitted -->和资格迹<!-- raw HTML omitted -->
<ul>
<li>
<!-- raw HTML omitted -->
</li>
<li>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>资格迹本质上是记录了所有状态s对后继状态<!-- raw HTML omitted -->的贡献度，被用来对TD误差进行加权</li>
</ul>
<h2 id="总结">总结</h2>
<ul>
<li>前向视角
<ul>
<li>利用t+1,t+2,&hellip;t+n时刻的V(s)求解t时刻的V(s)</li>
<li>容易理解</li>
<li>需要拥有完整的状态转移片段才能求解，跟MC一样离线更新</li>
</ul>
</li>
<li>后向视角
<ul>
<li>利用t+1时刻的V(s)更新t,t-1,t-2,t-3,&hellip;0时刻的V(s)</li>
<li>在线更新，每一个时刻都更新一遍之前所有时刻的V</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="n步sarsa">n步Sarsa</h1>
              </div>
            </div>

            <div class="f6 text-gray mt-2">
              

              Created
              <relative-time datetime=" Mon, 01 Jan 0001 00:00:00 &#43;0000" class="no-wrap"
                title=" Mon, 01 Jan 0001 00:00:00 &#43;0000">
                Mon, 01 Jan 0001 00:00:00 &#43;0000
              </relative-time>
            </div>
          </div>
        </li>
        
      </ul>
      <div class="paginate-container">
        <div class="BtnGroup">
          
          <button class="btn btn-outline BtnGroup-item" disabled="disabled">Previous</button>
          
          
          <a rel="nofollow" class="btn btn-outline BtnGroup-item" href="/categories/reinforcement-learning/page/2/">Next</a>
          
        </div>
      </div>
    </div>
  </div>
</div>

      
    </div>
  </div>
</div>
<script>
window.onscroll = function (e) {
  const headerImg = document.querySelector('#headerImg');
  const headerStuck = document.querySelector('#headerStuck');
  if (headerImg.getBoundingClientRect().bottom <= 0) {
    headerStuck.classList.add('is-stuck');
    if (window.innerWidth >= 1280) {
      headerStuck.setAttribute('style', 'top: 12px;')
    } else {
      headerStuck.setAttribute('style', 'top: 0;')
    }
  } else {
    headerStuck.classList.remove('is-stuck');
  }
};

var style = localStorage.getItem('data-color-mode')
githubIconElement = document.getElementById('github-icon')
twitterIconElement = document.getElementById('twitter-icon')
if (style == 'light') {
  if (githubIconElement) githubIconElement.setAttribute('fill', '#24292e')
  if (twitterIconElement) twitterIconElement.setAttribute('fill', 'black')
}
else {
  if (githubIconElement) {
    githubIconElement.removeAttribute('fill')
    githubIconElement.setAttribute('class', 'octicon')
    githubIconElement.setAttribute('color', '#f0f6fc')
  }
  if (twitterIconElement)  twitterIconElement.setAttribute('fill', 'white') 
}
</script>

  

</div>

  <div class="footer container-xl width-full p-responsive">
  <div
    class="position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between flex-sm-items-center pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light ">
    <a aria-label="Homepage" title="GitHub" class="footer-octicon d-none d-lg-block mr-lg-4" href="http://localhost:1313/">
      <svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="24">
        <path fill-rule="evenodd"
          d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z">
        </path>
      </svg>
    </a>
    <ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0">
      
      <li class="mr-3 mr-lg-0">Theme by <a href='https://github.com/MeiK2333/github-style'>github-style</a></li>
      
      <li class="mr-3 mr-lg-0">GitHub and the Invertocat logo are trademarks of <a href="https://github.com/">GitHub, Inc.</a></li>
    </ul>
  </div>
  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 text-gray-light"></span>
  </div>


</div>

</body>

<script type="application/javascript" src="http://localhost:1313/js/github-style.js"></script>







</html>