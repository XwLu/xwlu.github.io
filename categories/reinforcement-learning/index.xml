<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement-Learning on My New Hugo Site</title>
    <link>http://localhost:1313/categories/reinforcement-learning/</link>
    <description>Recent content in Reinforcement-Learning on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/categories/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Common Knowledges on Reinforcement Learning</title>
      <link>http://localhost:1313/post/reinforcement-learning/common/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/common/</guid>
      <description>&lt;h1 id=&#34;问题定义&#34;&gt;问题定义&lt;/h1&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;每个时刻&amp;lt;img src=&amp;#34;https://latex.codecogs.com/svg.image?t&amp;#34;/&amp;gt;&#xA;- 智能体(Agent)&#xA;  - 智能体执行动作&amp;lt;img src=&amp;#34;https://latex.codecogs.com/svg.image?A_{t}&amp;#34;/&amp;gt;，并在环境中得到观测&amp;lt;img src=&amp;#34;https://latex.codecogs.com/svg.image?O_{t}&amp;#34;/&amp;gt;和奖励&amp;lt;img src=&amp;#34;https://latex.codecogs.com/svg.image?R_{t}&amp;#34;/&amp;gt;&#xA;- 环境(Environment)&#xA;  - 环境会对智能体的动作&amp;lt;img src=&amp;#34;https://latex.codecogs.com/svg.image?A_{t}&amp;#34;/&amp;gt;的做出反应,然后发送新的观测&amp;lt;img src=&amp;#34;https://latex.codecogs.com/svg.image?O_{t+1}&amp;#34;/&amp;gt;和奖励&amp;lt;img src=&amp;#34;https://latex.codecogs.com/svg.image?R_{t+1}&amp;#34;/&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;核心概念&#34;&gt;核心概念&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;智能体&#34;&gt;智能体&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;智能体是指强化学习需要优化的部分,是我们能够精确控制的部分&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;环境&#34;&gt;环境&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;环境是我们不能直接控制的部分&lt;/li&gt;&#xA;&lt;li&gt;环境并不是指自然环境，不同的问题,智能体和环境的划分也有所区别&#xA;&lt;ul&gt;&#xA;&lt;li&gt;机器人探索房间 vs. 机器人行走控制&lt;/li&gt;&#xA;&lt;li&gt;仿真环境中的控制 vs. 实际环境中的控制&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;区分智能体和环境是强化学习的第一步&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;奖励reward&#34;&gt;奖励(Reward)&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;奖励是强化学习的核心&lt;/li&gt;&#xA;&lt;li&gt;可以没有观测,但是不能没有奖励&lt;/li&gt;&#xA;&lt;li&gt;奖励是强化学习区别其他机器学习的标志特征&lt;/li&gt;&#xA;&lt;li&gt;特点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;奖励&lt;!-- raw HTML omitted --&gt;是一个&lt;strong&gt;标量&lt;/strong&gt;反馈&lt;/li&gt;&#xA;&lt;li&gt;它衡量了智能体在时间&lt;!-- raw HTML omitted --&gt;上做得有多好&lt;/li&gt;&#xA;&lt;li&gt;智能体的目标就是&lt;strong&gt;最大化累计奖励&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;强化学习组成&#34;&gt;强化学习组成&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;奖励&#34;&gt;奖励&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;指智能体在执行某个动作&lt;!-- raw HTML omitted --&gt;后得到的累计回报&lt;!-- raw HTML omitted --&gt;，&lt;!-- raw HTML omitted --&gt;，&lt;!-- raw HTML omitted --&gt;。其中&lt;!-- raw HTML omitted --&gt;越小表示我们越关注短期奖励，&lt;!-- raw HTML omitted --&gt;越大表示我们越关注长期奖励&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Q Learning</title>
      <link>http://localhost:1313/post/reinforcement-learning/dqn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/dqn/</guid>
      <description>&lt;h1 id=&#34;基于值函数的深度学习网络&#34;&gt;基于值函数的深度学习网络&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;dqn&#34;&gt;DQN&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;深度强化学习的鼻祖式工作&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;解决了两个问题&#34;&gt;解决了两个问题&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一段时间内的训练数据具有较强的相关性，如果按照顺序进行训练，会对当前状态下的情况产生过拟合。通过将历史训练数据存在一个buffer里，通过随机抽取的方式进行训练，解决了这一问题。&lt;/li&gt;&#xA;&lt;li&gt;首次证明了能够通过raw pixels解决游戏问题，对所有游戏通用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;关键特点&#34;&gt;关键特点&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Q Learning + DNN&lt;/li&gt;&#xA;&lt;li&gt;Experience Replay&lt;/li&gt;&#xA;&lt;li&gt;Target Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法流程&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/dqn-flow.png?raw=true&#34; alt=&#34;dqn&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;double-dqn&#34;&gt;Double DQN&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;核心思路&#34;&gt;核心思路&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DQN中的TD目标值&lt;!-- raw HTML omitted --&gt;存在max操作，会引入一个正向偏差&lt;/li&gt;&#xA;&lt;li&gt;因此建模两个Q网络，一个用于选动作，一个用于评估动作：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;其实只要把DQN的target network也变成独立更新的就行了&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法流程-1&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/double-dqn-flow.png?raw=true&#34; alt=&#34;double-dqn&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;dueling-network&#34;&gt;Dueling Network&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;核心思路-1&#34;&gt;核心思路&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/dueling-dqn.png?raw=true&#34; alt=&#34;dueling-dqn&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;用一个网络，分别学习V函数和A(优势)函数，最后相加得到Q函数&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;优势&#34;&gt;优势&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于很多状态，不需要估计每个动作的Q值，每来一个样本都可以更新一次V函数，V函数的学习机会比Q函数高很多，- 泛化性能好，当新的动作进来时，不需要重新学习V，只需要重新学习A&lt;/li&gt;&#xA;&lt;li&gt;减少了Q函数由于状态和动作维度差导致的噪声和突变&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;prioritized-experience-replay&#34;&gt;Prioritized Experience Replay&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;核心思路-2&#34;&gt;核心思路&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DQN是从memory中均匀的采样，有时候，我们希望更多的去采样对学习有帮助的片段，给不同的experience提供不同的权重&lt;/li&gt;&#xA;&lt;li&gt;利用TD误差去衡量权重&lt;/li&gt;&#xA;&lt;li&gt;需要使用sum-tree以及binary heap data structure去实现&lt;/li&gt;&#xA;&lt;li&gt;为了保证新加入的样本至少被采样一次，新样本的TD误差会被设置为最大&lt;/li&gt;&#xA;&lt;li&gt;类似于DP中的优先清理&lt;/li&gt;&#xA;&lt;li&gt;Experience Replay 使得更新不受限于实际经验的顺序&lt;/li&gt;&#xA;&lt;li&gt;Prioritized Experience Replay 使得更新不受限于实际经验的频率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;存在问题&#34;&gt;存在问题&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TD 误差对噪声敏感&lt;/li&gt;&#xA;&lt;li&gt;TD 误差小的 transition 长时间不更新&lt;/li&gt;&#xA;&lt;li&gt;过分关注 TD 误差大的 transition 丧失了样本多样性 使用某种分布采样了 Experience, 会引入 Bias&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;解决方法&#34;&gt;解决方法&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;两种变体：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;其中，&lt;!-- raw HTML omitted --&gt;表示TD误差，&lt;!-- raw HTML omitted --&gt;表示人为施加的噪声，通过这个噪声保证多样性&lt;/li&gt;&#xA;&lt;li&gt;&lt;!-- raw HTML omitted --&gt;，序号的倒数来表示权重，对噪声的敏感度下降。简单的说，即使TD误差有0.1的误差，但顺序上依然是排第二，这时，误差就没有影响了。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;重要性采样，消除Bias&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;算法流程-2&#34;&gt;算法流程&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/prioritized-exp-replay.png?raw=true&#34; alt=&#34;prioritized-exp-replay&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;rainbow&#34;&gt;Rainbow&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;核心思想&#34;&gt;核心思想&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将大量的前人的工作汇总实现，明确每一种改进所对应的效果，采用的工作有&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DQN&lt;/li&gt;&#xA;&lt;li&gt;Double DQN&lt;/li&gt;&#xA;&lt;li&gt;Dueling DQN&lt;/li&gt;&#xA;&lt;li&gt;Prioritized Experience Replay&lt;/li&gt;&#xA;&lt;li&gt;NoiseNet(Noisy Netwoks for Exploration, AAAI2018)&lt;/li&gt;&#xA;&lt;li&gt;Distributional RL(A Distributional Perspective on Reinforcement Learning, 2017)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;效果&#34;&gt;效果&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rainbow的效果比所有的base line好&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/rainbow.png?raw=true&#34; alt=&#34;Rainbow&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Dynamic Planning</title>
      <link>http://localhost:1313/post/reinforcement-learning/dynamic-planning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/dynamic-planning/</guid>
      <description>&lt;h1 id=&#34;概念定义&#34;&gt;概念定义&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;当有一个精确的环境模型时,可以用动态规划去解&lt;/li&gt;&#xA;&lt;li&gt;将问题分解成子问题,通过解决子问题,来解决原问题&lt;/li&gt;&#xA;&lt;li&gt;贝尔曼方程是关键&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;问题特性&#34;&gt;问题特性&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;最优子结构&#34;&gt;最优子结构&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;满足最优性原理：不论初始状态和初始决策如何,对于前面决策所造成的某一状态而言,其后各阶段的决策序列必须构成最优策略&lt;/li&gt;&#xA;&lt;li&gt;最优的解可以被分解成子问题的最优解&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;交叠式子问题&#34;&gt;交叠式子问题&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;子问题能够被多次重复&lt;/li&gt;&#xA;&lt;li&gt;子问题的解要能够被缓存并再利用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;策略评价&#34;&gt;策略评价&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;给定一个策略&lt;!-- raw HTML omitted --&gt;,求对应的值函数&lt;!-- raw HTML omitted --&gt;或者&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;方法&#34;&gt;方法&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接解&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可以直接求得精确解&lt;/li&gt;&#xA;&lt;li&gt;时间复杂度&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;迭代解&#xA;&lt;ul&gt;&#xA;&lt;li&gt;利用贝尔曼期望方程迭代求解&lt;/li&gt;&#xA;&lt;li&gt;可以收敛到精确解&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;策略提升&#34;&gt;策略提升&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;根据现有的策略评价结果&lt;!-- raw HTML omitted --&gt;改进策略&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;典型的如贪婪策略提升&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;策略迭代&#34;&gt;策略迭代&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;流程&#34;&gt;流程&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;{策略评价(迭代k次，直到接近收敛为止) + 策略提升(提升1次)}; &amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;v1 → π1 → v2 → π2 → v3 → π3 → &amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;终止条件&#34;&gt;终止条件&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;提升停止&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;特点&#34;&gt;特点&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;有显式的策略&lt;/li&gt;&#xA;&lt;li&gt;迭代过程中的值函数对应了某个具体的策略&lt;/li&gt;&#xA;&lt;li&gt;效率较低&lt;/li&gt;&#xA;&lt;li&gt;贝尔曼期望方程 + 贪婪策略提升&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;值迭代&#34;&gt;值迭代&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;流程-1&#34;&gt;流程&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;策略评价(迭代1次); &amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;v1 → v2 → v3 → &amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;特点-1&#34;&gt;特点&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;没有显式的策略&lt;/li&gt;&#xA;&lt;li&gt;迭代过程中的值函数可能不对应任何策略&lt;/li&gt;&#xA;&lt;li&gt;效率较高&lt;/li&gt;&#xA;&lt;li&gt;贝尔曼期望方程&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;异步动态规划&#34;&gt;异步动态规划&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;以某种顺序单独考虑每一个状态&lt;/li&gt;&#xA;&lt;li&gt;能够大大减少计算量&lt;/li&gt;&#xA;&lt;li&gt;只要所有的状态都能被持续的选择到,收敛性能够保证&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;常用的三种形式&#34;&gt;常用的三种形式&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;就地(In-Place)动态规划&lt;/li&gt;&#xA;&lt;li&gt;优先清理&lt;/li&gt;&#xA;&lt;li&gt;实时动态规划&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;就地in-place动态规划&#34;&gt;就地(In-Place)动态规划&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;同步值迭代存储了两个版本的值函数，在计算&lt;!-- raw HTML omitted --&gt;的时候，使用了&lt;!-- raw HTML omitted --&gt;的复制版本，在整个更新过程中，&lt;!-- raw HTML omitted --&gt;是不变的，保持上一个循环的状态。&lt;/li&gt;&#xA;&lt;li&gt;就地(In-Place)动态规划只对一个值函数&lt;!-- raw HTML omitted --&gt;进行更新，因此，从左上角开始更新和从右下角开始更新，得到的结果是不一样的。&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Markov Decision Processes</title>
      <link>http://localhost:1313/post/reinforcement-learning/markov-decision-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/markov-decision-processes/</guid>
      <description>&lt;h1 id=&#34;mdps&#34;&gt;MDPs&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;马尔可夫性&#34;&gt;马尔可夫性&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;只要知道现在，将来和过去条件独立&lt;/li&gt;&#xA;&lt;li&gt;每一时刻的状态只与上一时刻的状态有关&lt;/li&gt;&#xA;&lt;li&gt;当前状态包含了所有的历史状态信息&lt;/li&gt;&#xA;&lt;li&gt;要求环境全观测&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;任务类型定义&#34;&gt;任务类型定义&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;强化学习中,从初始状态&lt;!-- raw HTML omitted --&gt;到终止状态的序列过程,被称为一个片段(episode)。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果一个任务总以终止状态结束,那么这个任务被称为片段任务(episodic task)&lt;/li&gt;&#xA;&lt;li&gt;如果一个任务会没有终止状态,会被无限执行下去,这被称为连续性任务 (continuing task)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;终止状态等价于自身转移概率为 1,奖励为 0 的的状态&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;状态转移矩阵&#34;&gt;状态转移矩阵&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;s1&lt;/th&gt;&#xA;          &lt;th&gt;s2&lt;/th&gt;&#xA;          &lt;th&gt;s3&lt;/th&gt;&#xA;          &lt;th&gt;s4&lt;/th&gt;&#xA;          &lt;th&gt;转移&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;0.5&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;0.5&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;s1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;0.1&lt;/td&gt;&#xA;          &lt;td&gt;0.2&lt;/td&gt;&#xA;          &lt;td&gt;0.3&lt;/td&gt;&#xA;          &lt;td&gt;0.4&lt;/td&gt;&#xA;          &lt;td&gt;s2&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;1.0&lt;/td&gt;&#xA;          &lt;td&gt;s3&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;          &lt;td&gt;1.0&lt;/td&gt;&#xA;          &lt;td&gt;s4&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;上图中s1转换到s1的概率是0.5，转换到s3的概率是0.5；s2转换到s1的概率是0.1，转换到s2的概率是0.2，转换到s3的概率是0.3，转换到s4的概率是0.4。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;奖励与回报&#34;&gt;奖励与回报&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;奖励值对每一个状态的评价&#34;&gt;奖励值:对每一个状态的评价&lt;/h3&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;回报值-对每一个片段的评价&#34;&gt;回报值: 对每一个片段的评价&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于片断性任务，回报值是未来有限个状态的奖励值的和&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;对于连续性任务，回报值是未来无限个状态的奖励值的和&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;回报值是从时间&lt;!-- raw HTML omitted --&gt;处开始的累计衰减奖励&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;指数衰减值&#34;&gt;指数衰减值&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对未来的把握也是逐渐衰减的&lt;/li&gt;&#xA;&lt;li&gt;一般情况下,我们更关注短时间的反馈&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;值函数某个状态所对应回报值的期望&#34;&gt;值函数:某个状态所对应回报值的期望&lt;/h3&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;贝尔曼方程&#34;&gt;贝尔曼方程&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;强化学习的核心&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;策略&#34;&gt;策略&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;状态值函数(&lt;!-- raw HTML omitted --&gt;)：是从状态 s 开始，使用策略 π 得到的期望回报值&lt;/li&gt;&#xA;&lt;li&gt;状态动作值函数(&lt;!-- raw HTML omitted --&gt;)：是从状态 s 开始,执行动作 a，然后使用策略 π 得到的期望回报值&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;知识点&#34;&gt;知识点&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;贝尔曼最优方程不是线性的&lt;/li&gt;&#xA;&lt;li&gt;一般很难有闭式的解&lt;/li&gt;&#xA;&lt;li&gt;可以使用迭代优化的方法去解&#xA;&lt;ul&gt;&#xA;&lt;li&gt;值迭代&lt;/li&gt;&#xA;&lt;li&gt;策略迭代&lt;/li&gt;&#xA;&lt;li&gt;Q 学习&lt;/li&gt;&#xA;&lt;li&gt;SARSA&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;pomdp&#34;&gt;POMDP&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;观测不等于状态O ≠ S&lt;/li&gt;&#xA;&lt;li&gt;POMDPs 由七元组构成 &amp;lt; S, A, O, P, R, Z, γ &amp;gt;&lt;/li&gt;&#xA;&lt;li&gt;Z是观测函数&lt;/li&gt;&#xA;&lt;li&gt;观测不满足马尔可夫性,因此也不满足贝尔曼方程&lt;/li&gt;&#xA;&lt;li&gt;状态未知,隐马尔可夫过程&lt;/li&gt;&#xA;&lt;li&gt;有时对于 POMDPs 来说,最优的策略是随机性的&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;无衰减-mdps&#34;&gt;无衰减 MDPs&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用于各态历经马尔可夫决策过程&lt;/li&gt;&#xA;&lt;li&gt;存在独立于状态的平均奖赏&lt;/li&gt;&#xA;&lt;li&gt;求值函数时,需要减去该平均奖赏,否则有可能奖赏爆炸&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Model based RL</title>
      <link>http://localhost:1313/post/reinforcement-learning/model-based-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/model-based-rl/</guid>
      <description>&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;强化学习可以分为&lt;strong&gt;有模型&lt;/strong&gt;和&lt;strong&gt;无模型&lt;/strong&gt;的方法两大类&lt;/li&gt;&#xA;&lt;li&gt;未知模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;学习法&lt;/li&gt;&#xA;&lt;li&gt;通过智能体的交互，学习值函数和策略&lt;/li&gt;&#xA;&lt;li&gt;代表方法：MC，TD&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;已知模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;规划法&lt;/li&gt;&#xA;&lt;li&gt;无需智能体交互，直接从模型学习最优策略&lt;/li&gt;&#xA;&lt;li&gt;代表方法：DP&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;基于模型的强化学习&#34;&gt;基于模型的强化学习&lt;/h1&gt;&#xA;&lt;h2 id=&#34;核心思路&#34;&gt;核心思路&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通过经验，学习出一个虚拟的环境模型&lt;/li&gt;&#xA;&lt;li&gt;利用学到的环境模型，进行动态规划，计算价值函数或者策略&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;优势&#34;&gt;优势&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可以通过监督学习，有效地学习环境模型&lt;/li&gt;&#xA;&lt;li&gt;可以将学到的环境模型放在GPU内，快速得到大量的交互信息&lt;/li&gt;&#xA;&lt;li&gt;没有任何真实损失&lt;/li&gt;&#xA;&lt;li&gt;直接利用环境模型的不确定性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;劣势&#34;&gt;劣势&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;先学环境模型，再学值函数，存在两次近似误差 -&amp;gt; 累计误差&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;dyna-q&#34;&gt;Dyna-q&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;算法流程&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/dyna-q-flow.png?raw=true&#34; alt=&#34;流程图&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;repeat n times的部分实际上是在用学习到的虚拟环境模型进行学习，这一部分可以新开一个线程，和外面真实的环境交互分开。&lt;/li&gt;&#xA;&lt;li&gt;真实的环境交互1秒一次，而计算机中虚拟的环境交互0.1秒一次，那这里的n就是10。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;蒙特卡洛树搜索monte-carlo-tree-search&#34;&gt;蒙特卡洛树搜索Monte-Carlo Tree Search&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;特性&#34;&gt;特性&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;适用于Combinatorial Games&lt;/li&gt;&#xA;&lt;li&gt;Combinatorial Games特点：零和、完美信息、确定性、离散、序列化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法核心&#34;&gt;算法核心&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在MCTS中，仿真策略需要策略提升&lt;/li&gt;&#xA;&lt;li&gt;每次仿真有两个阶段&#xA;&lt;ul&gt;&#xA;&lt;li&gt;树策略(提升)：选择动作，最大化Q(S, A)&lt;/li&gt;&#xA;&lt;li&gt;默认策略(固定)：快速计算到终止状态&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Repeat(每次仿真)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用MC评价来估计Q(S, A)&lt;/li&gt;&#xA;&lt;li&gt;提升树策略，比如epsilon-贪婪，UCB等&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;对仿真出来的经验做MC优化&lt;/li&gt;&#xA;&lt;li&gt;搜索到最优的搜索树Q(S,A)-&amp;gt;q*(S,A)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;epsilon-贪婪对于非最优策略是均匀采样的&#xA;UCB既考虑了值函数，又考虑了探索的次数 = 回报值/仿真次数&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;优势-1&#34;&gt;优势&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Highly selective best-first search&lt;/li&gt;&#xA;&lt;li&gt;动态评价状态&lt;/li&gt;&#xA;&lt;li&gt;结合了采样去打破维度诅咒，用采样取代了暴力搜索&lt;/li&gt;&#xA;&lt;li&gt;适合于各种黑盒模型，不需要满足马尔可夫性&lt;/li&gt;&#xA;&lt;li&gt;计算有效，容易并行&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法流程&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-1.png?raw=true&#34; alt=&#34;MCTS1&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;当前状态S1为叶子节点，直接从当前状态开始，用默认策略进行仿真&lt;/li&gt;&#xA;&lt;li&gt;结果“赢了”&lt;/li&gt;&#xA;&lt;li&gt;更新节点的UCB=获胜次数/仿真次数&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-2.png?raw=true&#34; alt=&#34;MCTS2&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;依据UCB规则选择一个动作向下走，每一个动作都对应了一个UCB&lt;/li&gt;&#xA;&lt;li&gt;UCB其实是一个值，与值函数正相关，与仿真次数负相关&lt;/li&gt;&#xA;&lt;li&gt;但是目前仿真次数都是0，所以根据值函数选一个动作，到达S2&lt;/li&gt;&#xA;&lt;li&gt;结果“输了”&lt;/li&gt;&#xA;&lt;li&gt;更新树策略路径上的每个节点的UCB&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-3.png?raw=true&#34; alt=&#34;MCTS3&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从S1开始，根据UCB选一个动作&lt;/li&gt;&#xA;&lt;li&gt;由于第二步中，最终结果是“输了”，所以第二步中动作的UCB下降了，于是从S1重新选一个动作，到达S3&lt;/li&gt;&#xA;&lt;li&gt;结果“赢了”&lt;/li&gt;&#xA;&lt;li&gt;更新树策略路径上的每个节点的UCB&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-4.png?raw=true&#34; alt=&#34;MCTS4&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;再从S1开始搜索，搜到S3后，再选一个动作，到达S4&lt;/li&gt;&#xA;&lt;li&gt;结果“输了”&lt;/li&gt;&#xA;&lt;li&gt;更新树策略路径上的每个节点的UCB&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-5.png?raw=true&#34; alt=&#34;MCTS5&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;再从S1开始搜索，搜到S3后，没有去S4(因为S4刚刚输了，UCB下降)，再选一个动作，到达S5&lt;/li&gt;&#xA;&lt;li&gt;结果“赢了”&lt;/li&gt;&#xA;&lt;li&gt;更新树策略路径上的每个节点的UCB&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;后面的步骤依次类推&#34;&gt;后面的步骤依次类推&lt;/h4&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/model-based/mcts-flow.png?raw=true&#34; alt=&#34;MCTS-FLOW&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;td搜索&#34;&gt;TD搜索&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法特性&#34;&gt;算法特性&lt;/h3&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;有些情况下，没有终止状态，MC方法不适用&lt;/li&gt;&#xA;&lt;li&gt;将MCTS中的MC评价换成TD评价&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法流程-1&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从当前状态St开始采样片段&lt;/li&gt;&#xA;&lt;li&gt;估计Q(s, a)&lt;/li&gt;&#xA;&lt;li&gt;对于每一步的仿真，使用Sarsa方法更新Q函数&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;基于Q(s, a)选择动作&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Monte-Carlo Sampling</title>
      <link>http://localhost:1313/post/reinforcement-learning/monte-carlo-sampling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/monte-carlo-sampling/</guid>
      <description>&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;蒙特卡洛采样是无模型方法&lt;/li&gt;&#xA;&lt;li&gt;行为策略是智能体与环境交互的策略&lt;/li&gt;&#xA;&lt;li&gt;目标策略是我们要学习的策略&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;在策略on-policy学习&#34;&gt;在策略（on-policy）学习&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;行为策略和目标策略是同一个策略&lt;/li&gt;&#xA;&lt;li&gt;直接使用样本统计属性去估计总体&lt;/li&gt;&#xA;&lt;li&gt;更简单,且收敛性更好&lt;/li&gt;&#xA;&lt;li&gt;数据利用性更差(只有智能体当前交互的样本能够被利用)&lt;/li&gt;&#xA;&lt;li&gt;限定了学习过程中的策略是随机性策略&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;离策略off-policy学习&#34;&gt;离策略（off-policy）学习&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;行为策略和目标策略不是同一个策略&lt;/li&gt;&#xA;&lt;li&gt;一般行为策略选用随机性策略，目标策略选用确定性策略&lt;/li&gt;&#xA;&lt;li&gt;需要结合重要性采样才能使用样本估计总体&lt;/li&gt;&#xA;&lt;li&gt;方差更大,收敛性更差&lt;/li&gt;&#xA;&lt;li&gt;数据利用性更好 (可以使用其他智能体交互的样本)&lt;/li&gt;&#xA;&lt;li&gt;行为策略需要比目标策略更具备探索性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;重要性采样&#34;&gt;重要性采样&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;是一种估计概率分布期望值的技术,它使用了来自其他概率分布的样本&lt;/li&gt;&#xA;&lt;li&gt;主要用于无法直接采样原分布的情况&lt;/li&gt;&#xA;&lt;li&gt;估计期望值时,需要加权概率分布的比值&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;算法特性&#34;&gt;算法特性&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MC方法可以被用于任意涉及随机变量的估计&lt;/li&gt;&#xA;&lt;li&gt;这里MC方法特指利用统计平均估计期望值的方法&lt;/li&gt;&#xA;&lt;li&gt;MC方法从完整的片段中学习&lt;/li&gt;&#xA;&lt;li&gt;MC方法仅仅用于片段性任务(必须有终止条件)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;算法核心&#34;&gt;算法核心&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;通过不断的采样,然后统计平均回报值来估计值函数,方差较大&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从某个状态S开始，通过某种策略P进行探索，一直到终止状态，得到反馈Fk&lt;/li&gt;&#xA;&lt;li&gt;重复以上步骤n次，V(s)=(F1+F2+&amp;hellip;+Fn)/n&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;蒙特卡洛评价&#34;&gt;蒙特卡洛评价&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;首次拜访(First-visit)MC策略评价&lt;/li&gt;&#xA;&lt;li&gt;每次拜访(Every-visit)MC策略评价&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;s1,s2,s3,s1,s4,s2,s5 +1&#xA;s1,s2,s1,s5 +1&#xA;对于上面的两种采样轨迹，评价s1时，首次拜访只在s1在一条轨迹中第一次出现时N=N+1；每次拜访则是出现一次s1就N=N+1&#xA;首次拜访：(1+1)/2 = 1&#xA;每次拜访：(1+1)/4 = 0.25&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;q函数的mc方法&#34;&gt;Q函数的MC方法&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;每次是针对一个s和一个a进行评价&lt;/li&gt;&#xA;&lt;li&gt;为了充分探索所有的s,a组合，随机选择初始状态和初始动作&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;离策略的mc策略评价&#34;&gt;离策略的MC策略评价&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;核心是利用重要性采样去加权回报值&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;使用重要性采样会显著增加方差, 可能到无限大&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;mc小结&#34;&gt;MC小结&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;偏差为 0,是无偏估计&lt;/li&gt;&#xA;&lt;li&gt;方差较大,需要大量数据去消除&lt;/li&gt;&#xA;&lt;li&gt;收敛性较好&lt;/li&gt;&#xA;&lt;li&gt;没有利用马尔可夫性,有时可以用在非马尔可夫环境&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;增量式mc&#34;&gt;增量式MC&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;之前的蒙特卡洛算法需要采样大量轨迹之后再统一计算平均数，能不能在每一条轨迹之后都得到值函数的估计值呢?&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;这里的&lt;!-- raw HTML omitted --&gt;可以认为是更新的步长&lt;/li&gt;&#xA;&lt;li&gt;很多时候，我们会把&lt;!-- raw HTML omitted --&gt;替换为一个常数&lt;!-- raw HTML omitted --&gt;，好处如下：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;会逐渐遗忘过去的轨迹&lt;/li&gt;&#xA;&lt;li&gt;对初始值敏感度更小&lt;/li&gt;&#xA;&lt;li&gt;适用于不稳定环境&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;mc策略提升&#34;&gt;MC策略提升&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不能使用贪婪策略提升，会导致部分状态永远不会遍历到&lt;/li&gt;&#xA;&lt;li&gt;每次探索，有一定的几率随机选择动作，其他情况下都采取贪婪策略&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;无限探索下的极限贪婪glie&#34;&gt;无限探索下的极限贪婪(GLIE)&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;无限探索:所有的状态动作对能够被探索无穷次&lt;/li&gt;&#xA;&lt;li&gt;极限贪婪:在极限的情况下,策略会收敛到一个贪婪的策略&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;GLIE 蒙特卡洛优化能收敛到最优的 Q 函数&lt;/p&gt;</description>
    </item>
    <item>
      <title>Policy Gradient</title>
      <link>http://localhost:1313/post/reinforcement-learning/policy-gradient/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/policy-gradient/</guid>
      <description>&lt;h1 id=&#34;强化学习分类&#34;&gt;强化学习分类&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;策略梯度算法&#34;&gt;策略梯度算法&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接用神经网络表示策略&lt;/li&gt;&#xA;&lt;li&gt;神经网络输出N维的向量，每一维表示选择该动作的概率大小&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;值函数算法&#34;&gt;值函数算法&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用神经网络拟合Q或者V函数&lt;/li&gt;&#xA;&lt;li&gt;得到Q之后，利用贪婪策略等选择下一步动作&lt;/li&gt;&#xA;&lt;li&gt;&lt;!-- raw HTML omitted --&gt;或者&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;actor-critic&#34;&gt;Actor-Critic&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;学习值函数&lt;/li&gt;&#xA;&lt;li&gt;学习策略&lt;/li&gt;&#xA;&lt;li&gt;介于上面两者之间&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;策略梯度算法优缺点&#34;&gt;策略梯度算法优缺点&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;优点&#34;&gt;优点&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;更好的收敛性&lt;/li&gt;&#xA;&lt;li&gt;有效处理高维和连续的动作空间&lt;/li&gt;&#xA;&lt;li&gt;能够学到随机策略&lt;/li&gt;&#xA;&lt;li&gt;不会导致策略退化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;缺点&#34;&gt;缺点&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;容易陷入局部最优&lt;/li&gt;&#xA;&lt;li&gt;难以评价一个策略，评价结果方差很大&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;策略退化&#34;&gt;策略退化&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型的能力不够导致&lt;/li&gt;&#xA;&lt;li&gt;值函数估计不准导致&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;我们从经典的a2c算法入手讲解策略梯度算法&#34;&gt;我们从经典的A2C算法入手讲解策略梯度算法&lt;/h3&gt;&#xA;&lt;h1 id=&#34;优化目标&#34;&gt;优化目标&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;其中，&lt;!-- raw HTML omitted --&gt;表示策略网络的参数；&lt;!-- raw HTML omitted --&gt;表示一段状态转移轨迹；&lt;!-- raw HTML omitted --&gt;表示该轨迹的最终回报值；&lt;!-- raw HTML omitted --&gt;表示当策略网络的参数为&lt;!-- raw HTML omitted --&gt;时，出现&lt;!-- raw HTML omitted --&gt;的概率大小。&lt;/li&gt;&#xA;&lt;li&gt;在一个固定的环境再，一般来说，&lt;!-- raw HTML omitted --&gt;是稳定不变的。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;优化方法&#34;&gt;优化方法&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;梯度表达式&#34;&gt;梯度表达式&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;似然率角度梯度求解&#34;&gt;似然率角度梯度求解&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/likehood-solver.png?raw=true&#34; alt=&#34;LIKEHOOD&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;似然率梯度的理解&#34;&gt;似然率梯度的理解&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;!-- raw HTML omitted --&gt;是轨迹&lt;!-- raw HTML omitted --&gt;的出现概率随&lt;!-- raw HTML omitted --&gt;变化最陡的方向。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;沿正方向，轨迹出现的概率会变大&lt;/li&gt;&#xA;&lt;li&gt;沿负方向，轨迹出现的概率会变小&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;!-- raw HTML omitted --&gt;控制了参数更新的方向和步长，R是正的，就让轨迹出现的概率变大，并且R越大，步长的幅度越大；相反亦然。&lt;/li&gt;&#xA;&lt;li&gt;最终增大了高回报率轨迹出现的概率，减少了低回报率轨迹出现的概率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;轨迹分解到状态&#34;&gt;轨迹分解到状态&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/decompose.png?raw=true&#34; alt=&#34;decompose&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;算法流程&#34;&gt;算法流程&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/reinforce.png?raw=true&#34; alt=&#34;reinforce&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;actor-critic-1&#34;&gt;Actor-Critic&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;上面的reinforce算法中，&lt;!-- raw HTML omitted --&gt;的方差非常大，为了减小方差，我们引入了Critic函数&lt;!-- raw HTML omitted --&gt;代替&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;再进一步，由于每个Q都是正的，会导致网络对于任何轨迹都想提高其出现的概率，因此，引入一个基线。基线的选择为当前状态的V值。由此得到一个优势函数：&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;上面的方法需要设计一个Q函数一个V函数，为了简化，我们直接用TD误差代替优势函数。TD误差为：&lt;/li&gt;&#xA;&lt;li&gt;&lt;!-- raw HTML omitted --&gt;其中，&lt;!-- raw HTML omitted --&gt;是&lt;!-- raw HTML omitted --&gt;的后一个状态&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/conclusion.png?raw=true&#34; alt=&#34;reinforce&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;其中，Advantage Actor-Critic又叫A2C，由于TD Actor-Critic是Advantage Actor-Critic的无偏估计，所以实际在使用A2C的时候，都是用的TD Actor-Critic&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/pg/a2c.png?raw=true&#34; alt=&#34;A2C&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;A2C需要多进程来打破训练数据之间的相关性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;其他策略梯度算法简单介绍&#34;&gt;其他策略梯度算法简单介绍&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;确定性梯度策略算法dpg&#34;&gt;确定性梯度策略算法DPG&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;特性&#34;&gt;特性&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接采用确定性动作输出：&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;可以用于高维和连续动作的情况&lt;/li&gt;&#xA;&lt;li&gt;常规的策略梯度方法无法用到高维和连续动作空间&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;梯度求解&#34;&gt;梯度求解&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;过去一直认为无模型情况下确定性策略梯度不存在&lt;/li&gt;&#xA;&lt;li&gt;DPG证明了梯度存在，并建立了其与Q函数的关系&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/dpg-gradient.png?raw=true&#34; alt=&#34;dpg-gradient&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;ddpg&#34;&gt;DDPG&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;核心思路&#34;&gt;核心思路&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Continuous Control with Deep Reinforcement Learning (ICRL2016)&lt;/li&gt;&#xA;&lt;li&gt;结合了 DQN 和 DPG&lt;/li&gt;&#xA;&lt;li&gt;利用随机过程产生探索性动作&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/ddpg.png?raw=true&#34; alt=&#34;ddpg&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法流程-1&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/ddpg-flow.png?raw=true&#34; alt=&#34;ddpg-flow&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;a3c&#34;&gt;A3C&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;论文来源&#34;&gt;论文来源&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Asynchronous Methods for Deep Reinforcement Learning (ICML2016)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;问题提出&#34;&gt;问题提出&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Online 的算法和 DNN 结合后不稳定 (样本关联性)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;解决方案&#34;&gt;解决方案&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;创建多个agent，在多个环境中执行异步学习构建batch(多线程)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;来自不同环境的样本无相关性&lt;/li&gt;&#xA;&lt;li&gt;不依赖于 GPU 和大型分布式系统&lt;/li&gt;&#xA;&lt;li&gt;不同线程使用了不同的探索策略，增加了探索量&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;算法流程-2&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/a3c.png?raw=true&#34; alt=&#34;a3c&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;a2c&#34;&gt;A2C&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;来源&#34;&gt;来源&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OpenAI对A3C进行了改进，把&lt;strong&gt;异步&lt;/strong&gt;变成了&lt;strong&gt;同步&lt;/strong&gt;，等所有线程的动作执行完毕得到reward后一起拿来更新，可以用GPU完成该动作，效率高&lt;/li&gt;&#xA;&lt;li&gt;当batch_size较大时效果好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;策略梯度知识图谱&#34;&gt;策略梯度知识图谱&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/deep-rl/pg-graph.png?raw=true&#34; alt=&#34;pg-graph&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;扩展其他策略梯度算法&#34;&gt;扩展(其他策略梯度算法)&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;自然梯度法：寻找策略更新最快的方向&lt;/li&gt;&#xA;&lt;li&gt;信赖域策略优化算法(TRPO)：研究了更新步长的选择，步长选择在策略梯度中非常重要，但实现非常复杂&lt;/li&gt;&#xA;&lt;li&gt;近端策略优化(PPO)：对TRPO的改进，使实现非常简单，实际使用中，效果比较好甚至最好的方案&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Reinforcement Learning</title>
      <link>http://localhost:1313/post/reinforcement-learning/abstract/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/abstract/</guid>
      <description>&lt;h1 id=&#34;强化学习基础&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/common/&#34;&gt;强化学习基础&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;马尔科夫过程&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/markov-decision-processes/&#34;&gt;马尔科夫过程&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;动态规划&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/dynamic-planning/&#34;&gt;动态规划&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;无模型方法&#34;&gt;无模型方法&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;无模型方法：未知环境模型，需要通过和环境的交互获得反馈进行策略学习。有模型方法：已知环境模型，根据环境直接推导最优策略。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;蒙特卡洛&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/monte-carlo-sampling/&#34;&gt;蒙特卡洛&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;时间差分&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/temporal-difference/&#34;&gt;时间差分&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;多步自举&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/temporal-difference-lambda/&#34;&gt;多步自举&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;基于策略梯度的强化学习&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/policy-gradient&#34;&gt;基于策略梯度的强化学习&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;dqn&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/dqn&#34;&gt;DQN&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;基于信赖域的强化学习&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/trust-region-based-drl&#34;&gt;基于信赖域的强化学习&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;基于模型的强化学习&#34;&gt;&lt;a href=&#34;https://xwlu.github.io/wiki/reinforcement-learning/model-based-rl/&#34;&gt;基于模型的强化学习&lt;/a&gt;&lt;/h1&gt;</description>
    </item>
    <item>
      <title>Temporal Difference</title>
      <link>http://localhost:1313/post/reinforcement-learning/temporal-difference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/temporal-difference/</guid>
      <description>&lt;h1 id=&#34;temporal-difference&#34;&gt;Temporal Difference&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;增量式MC&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;时间差分TD&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;核心差别：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MC是根据[s1→终止状态]完整片段的最终回报更新s1的值函数&lt;/li&gt;&#xA;&lt;li&gt;TD是根据[s1→s2]这一步片段的即时回报值R和s2的估计值函数更新s1的值函数&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;与dp的对比&#34;&gt;与DP的对比&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DP是全宽备份&lt;/li&gt;&#xA;&lt;li&gt;TD是样本备份&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;td与mc的对比&#34;&gt;TD与MC的对比&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;TD 算法在知道结果之前学习&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TD算法在每一步之后都能在线学习&lt;/li&gt;&#xA;&lt;li&gt;MC算法必须等待最终回报值得到之后才能学习&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TD算法即便没有最终结果也能学习&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TD算法能够从不完整序列中学习&lt;/li&gt;&#xA;&lt;li&gt;MC算法仅仅能够从完整序列中学习&lt;/li&gt;&#xA;&lt;li&gt;TD算法适用于连续性任务和片段性任务&lt;/li&gt;&#xA;&lt;li&gt;MC算法仅仅适用于片段性任务&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TD算法有多个驱动力&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MC算法只有奖励值作为更新的驱动力&lt;/li&gt;&#xA;&lt;li&gt;TD算法有奖励值和状态转移作为更新的驱动力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;MC有高方差,零偏差&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;收敛性较好 (即使采用函数逼近)&lt;/li&gt;&#xA;&lt;li&gt;对初始值不太敏感&lt;/li&gt;&#xA;&lt;li&gt;随着样本数量的增加,方差逐渐减少, 趋近于 0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TD 有低方差,和一些偏差&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通常比 MC 效率更高&lt;/li&gt;&#xA;&lt;li&gt;表格法下TD(0)收敛到&lt;!-- raw HTML omitted --&gt;(函数逼近时不一定)&lt;/li&gt;&#xA;&lt;li&gt;对初始值更敏感&lt;/li&gt;&#xA;&lt;li&gt;随着样本数量的增加,偏差逐渐减少,趋近于 0&lt;/li&gt;&#xA;&lt;li&gt;样本数量有限时，TD的结果与真实结果的偏差比较稳定。MC可能出现巨大偏差。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TD要求环境符合马尔科夫性，MC不要求&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;自举和采样&#34;&gt;自举和采样&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;自举&lt;/strong&gt;: 使用随机变量的估计去更新&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MC 没有自举&lt;/li&gt;&#xA;&lt;li&gt;DP 和 TD 都有自举&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;采样&lt;/strong&gt;: 通过样本估计期望&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MC 和 TD 采样&lt;/li&gt;&#xA;&lt;li&gt;DP 不采样&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;td的优化方法&#34;&gt;TD的优化方法&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;整体思路是 策略评价+策略提升&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;策略评价&#34;&gt;策略评价&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;在策略评价sarsa&#34;&gt;在策略评价&lt;strong&gt;SARSA&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;公式&#34;&gt;公式&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;算法流程&#34;&gt;算法流程&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1:初始化&lt;!-- raw HTML omitted --&gt;且&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;2:repeat(对于每个片段)&lt;/li&gt;&#xA;&lt;li&gt;3:  初始化状态&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;4:  根据&lt;!-- raw HTML omitted --&gt;选择一个在&lt;!-- raw HTML omitted --&gt;处的动作&lt;!-- raw HTML omitted --&gt;(使用&lt;!-- raw HTML omitted --&gt;-贪婪策略)&lt;/li&gt;&#xA;&lt;li&gt;5:  repeat(对于片段中每一步)&lt;/li&gt;&#xA;&lt;li&gt;6:    执行动作&lt;!-- raw HTML omitted --&gt;，观测&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;7:    根据&lt;!-- raw HTML omitted --&gt;选择一个在&lt;!-- raw HTML omitted --&gt;处的动作&lt;!-- raw HTML omitted --&gt;(使用&lt;!-- raw HTML omitted --&gt;-贪婪策略)&lt;/li&gt;&#xA;&lt;li&gt;8:    &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;9:    &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;10:  until &lt;!-- raw HTML omitted --&gt;是终止状态&lt;/li&gt;&#xA;&lt;li&gt;11:until收敛&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;收敛性&#34;&gt;收敛性&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在满足以下条件时,Sarsa 算法收敛到最优的状态动作值函数&#xA;&lt;ul&gt;&#xA;&lt;li&gt;策略序列&lt;!-- raw HTML omitted --&gt;满足GLIE&lt;/li&gt;&#xA;&lt;li&gt;步长序列&lt;!-- raw HTML omitted --&gt;是一个Robbins-Monro序列&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;GLIE 保证了&#xA;&lt;ul&gt;&#xA;&lt;li&gt;充分的探索&lt;/li&gt;&#xA;&lt;li&gt;策略最终收敛到贪婪的策略&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Robbins-Monro保证了&#xA;&lt;ul&gt;&#xA;&lt;li&gt;步长足够大,足以克服任意初始值&lt;/li&gt;&#xA;&lt;li&gt;步长足够小,最终收敛 (常量步长不满足)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;期望sarsa&#34;&gt;期望SARSA&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;减少了由于&lt;!-- raw HTML omitted --&gt;的选择带来的方差&lt;/li&gt;&#xA;&lt;li&gt;在相同更新步数时,期望 Sarsa 比 Sarsa 的通用性更好&lt;/li&gt;&#xA;&lt;li&gt;可以在在策略和离策略中切换&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在策略:TD目标值中的&lt;!-- raw HTML omitted --&gt;中的策略&lt;!-- raw HTML omitted --&gt;和采样的策略是同一个策略&lt;/li&gt;&#xA;&lt;li&gt;离策略:TD目标值中的&lt;!-- raw HTML omitted --&gt;中的策略&lt;!-- raw HTML omitted --&gt;和采样的策略是不同的策略&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;一种特殊情况,TD目标值中的策略选择贪婪策略, 采样的策略选用ε-贪婪策略——&lt;strong&gt;Q学习&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;离策略评价q学习&#34;&gt;离策略评价&lt;strong&gt;Q学习&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;公式-1&#34;&gt;公式&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;算法流程-1&#34;&gt;算法流程&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1:初始化&lt;!-- raw HTML omitted --&gt;且&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;2:repeat(对于每个片段)&lt;/li&gt;&#xA;&lt;li&gt;3:  初始化状态&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;4:  repeat(对于片段中每一步)&lt;/li&gt;&#xA;&lt;li&gt;5:    根据&lt;!-- raw HTML omitted --&gt;选择一个在&lt;!-- raw HTML omitted --&gt;处的动作&lt;!-- raw HTML omitted --&gt;(使用&lt;!-- raw HTML omitted --&gt;-贪婪策略)&lt;/li&gt;&#xA;&lt;li&gt;6:    执行动作&lt;!-- raw HTML omitted --&gt;，观测&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;7:    &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;8:    &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;9:  until &lt;!-- raw HTML omitted --&gt;是终止状态&lt;/li&gt;&#xA;&lt;li&gt;10:until收敛&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;策略提升&#34;&gt;策略提升&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;-贪婪策略提升&#34;&gt;&lt;!-- raw HTML omitted --&gt;-贪婪策略提升&lt;/h2&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Temporal Difference Lambda</title>
      <link>http://localhost:1313/post/reinforcement-learning/temporal-difference-lambda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/temporal-difference-lambda/</guid>
      <description>&lt;h1 id=&#34;temporal-difference-lambda&#34;&gt;Temporal Difference Lambda&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;时间差分就是TD(0)算法，只向后采样一步；MC是向后采样整个片段；多步自举介于两者之间&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TD: &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;MC: &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;经验认为&lt;!-- raw HTML omitted --&gt;步左右，要好于TD(0)和MC&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;n步td策略评价&#34;&gt;n步TD策略评价&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;算法流程&#34;&gt;算法流程&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1:repeat(对于每一个片段)&lt;/li&gt;&#xA;&lt;li&gt;2:  repeat对于片段中的每一步&lt;/li&gt;&#xA;&lt;li&gt;3:    根据&lt;!-- raw HTML omitted --&gt;选择动作&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;4:    执行动作&lt;!-- raw HTML omitted --&gt;，观察到&lt;!-- raw HTML omitted --&gt;，并将其存储起来&lt;/li&gt;&#xA;&lt;li&gt;5:    if &lt;!-- raw HTML omitted --&gt;, then&lt;/li&gt;&#xA;&lt;li&gt;6:      &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;7:      if &lt;!-- raw HTML omitted --&gt;,then &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;8:      &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;9:    end if&lt;/li&gt;&#xA;&lt;li&gt;10:  until直到终止状态&lt;/li&gt;&#xA;&lt;li&gt;11:until收敛&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;两个注意点&#34;&gt;两个注意点&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;为了计算 n 步回报值,需要维护 R, S 的存储空间&lt;/li&gt;&#xA;&lt;li&gt;对于后继状态不足 n 个的,使用 MC 目标值&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;多步自举&#34;&gt;多步自举&lt;/h1&gt;&#xA;&lt;h2 id=&#34;前向视角&#34;&gt;前向视角&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;就是将TD(0),TD(1),&amp;hellip;,TD(n)求平均&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;!-- raw HTML omitted --&gt;，退化成TD(0)；&lt;!-- raw HTML omitted --&gt;，退化成MC&lt;/li&gt;&#xA;&lt;li&gt;&lt;!-- raw HTML omitted --&gt;更新公式&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;后向视角基于资格迹eligibility-traces&#34;&gt;后向视角&amp;ndash;基于资格迹(Eligibility Traces)&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;状态转移片段：s1→s1→s1→s2→s3&lt;/li&gt;&#xA;&lt;li&gt;信度分配(Credit assignment)问题:到底是s1还是s2造成了最后的s3&#xA;&lt;ul&gt;&#xA;&lt;li&gt;频率启发式: 归因到频数最高的状态&lt;/li&gt;&#xA;&lt;li&gt;近因启发式: 归因到最近的状态&lt;/li&gt;&#xA;&lt;li&gt;资格迹是两者的结合&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;资格迹的计算公式&#34;&gt;资格迹的计算公式&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;直观的感觉就是，第一次遇到这个状态s的时候，对它的记忆由0蹦到1，然后慢慢开始遗忘(对应&lt;!-- raw HTML omitted --&gt;这个动作)，下一次又遇到了，记忆就一下子清晰了(对应+1这个动作)，然后又慢慢遗忘。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;利用资格迹实现多步自举&#34;&gt;利用资格迹实现多步自举&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于每一个状态s，维护一个资格迹&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;更新值函数V(s)时，会更新每一个状态s&lt;/li&gt;&#xA;&lt;li&gt;使用TD误差&lt;!-- raw HTML omitted --&gt;和资格迹&lt;!-- raw HTML omitted --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;资格迹本质上是记录了所有状态s对后继状态&lt;!-- raw HTML omitted --&gt;的贡献度，被用来对TD误差进行加权&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;前向视角&#xA;&lt;ul&gt;&#xA;&lt;li&gt;利用t+1,t+2,&amp;hellip;t+n时刻的V(s)求解t时刻的V(s)&lt;/li&gt;&#xA;&lt;li&gt;容易理解&lt;/li&gt;&#xA;&lt;li&gt;需要拥有完整的状态转移片段才能求解，跟MC一样离线更新&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;后向视角&#xA;&lt;ul&gt;&#xA;&lt;li&gt;利用t+1时刻的V(s)更新t,t-1,t-2,t-3,&amp;hellip;0时刻的V(s)&lt;/li&gt;&#xA;&lt;li&gt;在线更新，每一个时刻都更新一遍之前所有时刻的V&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;n步sarsa&#34;&gt;n步Sarsa&lt;/h1&gt;</description>
    </item>
    <item>
      <title>Trust Region based DRL</title>
      <link>http://localhost:1313/post/reinforcement-learning/trust-region-based-drl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/reinforcement-learning/trust-region-based-drl/</guid>
      <description>&lt;h1 id=&#34;问题描述&#34;&gt;问题描述&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;策略梯度算法的更新步长很重要&#xA;&lt;ul&gt;&#xA;&lt;li&gt;步长太小，导致更新效率低下&lt;/li&gt;&#xA;&lt;li&gt;步长太大，导致参数更新的策略比上次更差，通过更差的策略采 样得到的样本更差，导致学习再次更新的参数会更差，最终崩溃&lt;/li&gt;&#xA;&lt;li&gt;如何选择一个合适的步长，使得每次更新得到的新策略所实现的回报 值单调不减&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;解决方案&#34;&gt;解决方案&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;信赖域 (Trust Region) 方法指在该区域内更新，策略所实现的回报 值单调不减&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;知识背景&#34;&gt;知识背景&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;自然梯度&#34;&gt;自然梯度&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Natural Gradient Works Efficiently in Learning, 1998&lt;/li&gt;&#xA;&lt;li&gt;在黎曼空间里面，最快的下降方向不是梯度方向，而是自然梯度方向&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;只有当坐标系统正交，才退化成欧式空间&lt;/li&gt;&#xA;&lt;li&gt;神经网络中的参数空间是黎曼空间&lt;/li&gt;&#xA;&lt;li&gt;其中&lt;!-- raw HTML omitted --&gt;为 Reimannian metric tensor&lt;/li&gt;&#xA;&lt;li&gt;统计问题中，&lt;!-- raw HTML omitted --&gt;可以用 Hessian 矩阵去计算&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;保守策略迭代&#34;&gt;保守策略迭代&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CPI: Approximately Optimal Approximate Reinforcement Learning, 2002&lt;/li&gt;&#xA;&lt;li&gt;给出策略性能增长的条件&#xA;&lt;ul&gt;&#xA;&lt;li&gt;策略更新后的所有优势函数非负&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;使用混合更新的方式更新策略&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;trpo&#34;&gt;TRPO&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Trust Region Policy Optimization, ICML2015&lt;/li&gt;&#xA;&lt;li&gt;以 CPI 为基础，推导出策略更新后性能的下界, 通过优化下界优化原函数&lt;/li&gt;&#xA;&lt;li&gt;实际操作时用 KL 散度作为约束&lt;/li&gt;&#xA;&lt;li&gt;求解带约束的优化问题时，利用自然梯度&lt;/li&gt;&#xA;&lt;li&gt;自然梯度需要求2阶导数，在大规模的神经网络里极其难求&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实际求解是利用了共轭梯度 + 线性搜索的方法, 避免求自然梯度&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;ppo&#34;&gt;PPO&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Proximal Policy Optimization Algorithms, 2017&lt;/li&gt;&#xA;&lt;li&gt;Openai blog(&lt;a href=&#34;https://blog.openai.com/openai-baselines-ppo/&#34;&gt;https://blog.openai.com/openai-baselines-ppo/&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;TRPO 太复杂，普通 PG 效果又不好&lt;/li&gt;&#xA;&lt;li&gt;PPO 本质上是 TRPO 的简化版&lt;/li&gt;&#xA;&lt;li&gt;移除了 KL 惩罚项和交替更新，把它变成了正则化项，写到目标函数里&lt;/li&gt;&#xA;&lt;li&gt;由于性能好，且容易实现，已经成为默认的 OPENAI 算法&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;知识图谱&#34;&gt;知识图谱&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/trpo/ppo-graph.png?raw=true&#34; alt=&#34;ppo&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;核心步骤&#34;&gt;核心步骤&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://github.com/XwLu/xwlu.github.io/blob/master/images/wiki/rl/trpo/ppo.png?raw=true&#34; alt=&#34;ppo&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;实现非常简单&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;其他信赖域算法&#34;&gt;其他信赖域算法&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;acktr&#34;&gt;ACKTR&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;acer&#34;&gt;ACER&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sample Efficient Actor-Critic with Experience Replay&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;gae&#34;&gt;GAE&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;High-Dimensional Continuous Control Using Generalized Advantage Estimation&lt;/li&gt;&#xA;&lt;li&gt;在估计advantage函数的时候，不是用传统的TD误差值去更新，而是用一种迭代的形式去更新&lt;/li&gt;&#xA;&lt;li&gt;基本上所有用到advantage的方法，用了这个trick之后，效果都会有所提升&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
